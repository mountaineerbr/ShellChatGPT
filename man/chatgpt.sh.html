<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="mountaineerbr" />
  <title>CHATGPT.SH(1) v0.83 | General Commands Manual</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">CHATGPT.SH(1) v0.83 | General Commands Manual</h1>
<p class="author">mountaineerbr</p>
<p class="date">November 2024</p>
</header>
<h3 id="name">NAME</h3>
<div class="line-block">   chatgpt.sh -- Wrapper for ChatGPT / DALL-E /
Whisper / TTS</div>
<h3 id="synopsis">SYNOPSIS</h3>
<div class="line-block">   <strong>chatgpt.sh</strong>
[<code>-cc</code>|<code>-d</code>|<code>-qq</code>] [<code>opt</code>..]
[<em>PROMPT</em>|<em>TEXT_FILE</em>|<em>PDF_FILE</em>]<br />
   <strong>chatgpt.sh</strong> <code>-i</code> [<code>opt</code>..]
[<em>X</em>|<em>L</em>|<em>P</em>][<em>hd</em>] [<em>PROMPT</em>]
#Dall-E-3<br />
   <strong>chatgpt.sh</strong> <code>-i</code> [<code>opt</code>..]
[<em>S</em>|<em>M</em>|<em>L</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-i</code> [<code>opt</code>..]
[<em>S</em>|<em>M</em>|<em>L</em>] [<em>PNG_FILE</em>]<br />
   <strong>chatgpt.sh</strong> <code>-i</code> [<code>opt</code>..]
[<em>S</em>|<em>M</em>|<em>L</em>] [<em>PNG_FILE</em>]
[<em>MASK_FILE</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-w</code> [<code>opt</code>..]
[<em>AUDIO_FILE</em>|<em>.</em>] [<em>LANG</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-W</code> [<code>opt</code>..]
[<em>AUDIO_FILE</em>|<em>.</em>] [<em>PROMPT-EN</em>]<br />
   <strong>chatgpt.sh</strong> <code>-z</code> [<code>opt</code>..]
[<em>OUTFILE</em>|<em>FORMAT</em>|<em>-</em>] [<em>VOICE</em>]
[<em>SPEED</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-ccWwz</code> [<code>opt</code>..]
-- [<em>PROMPT</em>] -- [<code>whisper_arg</code>..] --
[<code>tts_arg</code>..]<br />
   <strong>chatgpt.sh</strong> <code>-l</code> [<em>MODEL</em>]<br />
   <strong>chatgpt.sh</strong> <code>-TTT</code> [-v]
[<code>-m</code>[<em>MODEL</em>|<em>ENCODING</em>]]
[<em>INPUT</em>|<em>TEXT_FILE</em>|<em>PDF_FILE</em>]<br />
   <strong>chatgpt.sh</strong> <code>-HPP</code>
[<code>/</code><em>HIST_FILE</em>|<em>.</em>]<br />
   <strong>chatgpt.sh</strong> <code>-HPw</code></div>
<h3 id="description">DESCRIPTION</h3>
<p>This script acts as a wrapper for ChatGPT, DALL-E, Whisper, and TTS
endpoints from multiple service providers such as OpenAI, LocalAI,
Ollama, Anthropic, Mistral AI, GoogleAI, Groq AI, and GitHub Models.</p>
<p>With no options set, complete INPUT in single-turn mode of the native
chat completion.</p>
<p>Handles single-turn and multi-turn modes, pure text completions,
image generation/editing, speech-to-text, and text-to-speech.</p>
<p>Accepts prompts, text files, PDFs, and image files as input. Supports
various options for model selection, parameters, and output formats.</p>
<h4 id="chat-completion-modes">Chat Completion Modes</h4>
<p>Set <code>option -c</code> to start a multi-turn chat mode via
<strong>text completions</strong> with command line history support.
This option works with instruct models, defaults to
<em>gpt-3.5-turbo-instruct</em> if none set.</p>
<p>Set <code>options -cc</code> to start the chat mode via
<strong>native chat completions</strong>. This mode defaults to the
<em>gpt-4o</em> model, which is optimised to follow instructions. Try
<em>chatgpt-4o-latest</em> for a model optimised for chatting.</p>
<p>In chat mode, some options are automatically set to un-lobotomise the
bot.</p>
<p>Set <code>option -C</code> to <strong>resume</strong> (continue from)
last history session, and set <code>option -E</code> to exit on the
first response (even in multi turn mode).</p>
<h4 id="text-completion-modes">Text Completion Modes</h4>
<p><code>Option -d</code> starts a multi-turn session in <strong>plain
text completions</strong> with history support. This does not set
further options automatically, such as instructions or temperature.</p>
<p>To run the script in text completion in single turn mode, set a text
completion model such as <em>gpt-3.5-turbo-instruct</em> and other
options (such as temperature and stops) at the command line
invocation.</p>
<h4 id="insert-modes-fill-in-the-middle">Insert Modes
(Fill-In-the-Middle)</h4>
<p>Set <code>option -q</code> for <strong>insert mode</strong>. The flag
“<em>[insert]</em>” must be present in the middle of the input prompt.
Insert mode works completing between the end of the text preceding the
flag, and ends completion with the succeeding text after the flag.</p>
<p>Insert mode works with <code>instruct</code> and Mistral
<code>code</code> models.</p>
<h4 id="instruction-prompts">Instruction Prompts</h4>
<p>Positional arguments are read as a single <strong>PROMPT</strong>.
Model <strong>INSTRUCTION</strong> is optional but recommended and can
be set with <code>option -S</code>.</p>
<p><code>Option -S</code> sets an INSTRUCTION prompt (the initial
prompt) for text cmpls, and chat cmpls. A text file path may be supplied
as the single argument. Also see <em>CUSTOM / AWESOME PROMPTS</em>
section below.</p>
<p>In multi-turn interactions, prompts prefixed with a single colon
“<em>:</em>” are appended to the current request buffer as user messages
without making a new API call. Conversely, prompts starting with double
colons “<em>::</em>” are appended as instruction / system messages. For
text cmpls only, triple colons append the text immediately to the
previous prompt without a restart sequence.</p>
<p>To create and reuse a custom prompt, set the prompt name as a command
line option, such as “<code>-S .[_prompt_name_]</code>” or
“<code>-S ,[_prompt_name_]</code>”.</p>
<p>When the operator is a comma “<em>,</em>”, single-shot editing will
be available after loading the prompt text. Use “<em>,,</em>” to
actually edit the template file itself!</p>
<p>Note that loading a custom prompt will also change to its
respectively-named history file.</p>
<p>Alternatively, set the first positional argument with the operator
and the prompt name after any command line options, such as
“<code>.[_prompt_name_]</code>”. This loads the prompt file unless
instruction was set with command line options.</p>
<h4 id="commands">Commands</h4>
<p>If the first positional argument of the script starts with the
command operator forward slash “<code>/</code>” and a history file name,
the command “<code>/session</code> [<em>HIST_NAME</em>]” is assumed.
This will change to or create a new history file (with
<code>options -ccCdPP</code>).</p>
<p>If a plain text or PDF file path is set as the last positional
parameter or as an argument to <code>option -S</code> (set instruction
prompt), the file is loaded as text PROMPT.</p>
<p>With <strong>vision models</strong>, append the image file paths and
possibly URLs at the end of the prompt.</p>
<p>In chat mode, a PDF, DOC, TXT or URL filepath at the very end of the
user prompt is appended as text dump to the current user input.</p>
<p>Make sure file paths containing spaces are backslash-escaped.</p>
<h4 id="model-and-capacity">Model and Capacity</h4>
<p>Set model with “<code>-m</code> [<em>MODEL</em>]”, with
<em>MODEL</em> as its name, or set it as “<em>.</em>” to pick from the
model list. List available models with <code>option -l</code>.</p>
<p>Set <em>maximum response tokens</em> with <code>option</code>
“<code>-</code><em>NUM</em>” or “<code>-M</code> <em>NUM</em>”. This
defaults to <em>1024</em> tokens and <em>25000</em> for reasoning
models.</p>
<p>If a second <em>NUM</em> is given to this option, <em>maximum model
capacity</em> will also be set. The option syntax takes the form of
“<code>-</code><em>NUM/NUM</em>”, and “<code>-M</code>
<em>NUM-NUM</em>”.</p>
<p><em>Model capacity</em> (maximum model tokens) can be set more
intuitively with <code>option</code> “<code>-N</code> <em>NUM</em>”,
otherwise model capacity is set automatically for known models or to
<em>2048</em> tokens as fallback.</p>
<p>List models with <code>option -l</code> or run <code>/models</code>
in chat mode.</p>
<!--
Install models with `option -l` or command `/models`
and the `install` keyword.

Also supply a _model configuration file URL_ or,
if LocalAI server is configured with Galleries,
set "_\<GALLERY>_@_\<MODEL_NAME>_".
Gallery defaults to HuggingFace.

* NOTE: *  I recommend using LocalAI own binary to install the models!
-->
<!-- LocalAI only tested with text and chat completion models (vision) -->
<p><code>Option -y</code> sets python tiktoken instead of the default
script hack to preview token count. This option makes token count
preview accurate fast (we fork tiktoken as a coprocess for fast token
queries). Useful for rebuilding history context independently from the
original model used to generate responses.</p>
<h4 id="image-generations-and-edits-dall-e">Image Generations and Edits
(Dall-E)</h4>
<p><code>Option -i</code> <strong>generates images</strong> according to
text PROMPT. If the first positional argument is an <em>IMAGE</em> file,
then <strong>generate variations</strong> of it. If the first positional
argument is an <em>IMAGE</em> file and the second a <em>MASK</em> file
(with alpha channel and transparency), and a text PROMPT (required),
then <strong>edit the</strong> <em>IMAGE</em> according to <em>MASK</em>
and PROMPT. If <em>MASK</em> is not provided, <em>IMAGE</em> must have
transparency.</p>
<p>The <strong>size of output images</strong> may be set as the first
positional parameter in the command line: “<em>256x256</em>”
(<em>S</em>), “<em>512x512</em>” (<em>M</em>), “<em>1024x1024</em>”
(<em>L</em>), “<em>1792x1024</em>” (<em>X</em>), and
“<em>1024x1792</em>” (<em>P</em>).</p>
<p>The parameter “<em>hd</em>” may also be set for image quality
(<em>Dall-E-3</em>), such as “<em>Xhd</em>” or “<em>1792x1024hd</em>”.
Defaults=<em>1024x1024</em>.</p>
<p>For Dalle-3, optionally set the generation style as either
“<em>natural</em>” or “<em>vivid</em>” as a positional parameter in the
command line invocation.</p>
<p>See <strong>IMAGES section</strong> below for more information on
<strong>inpaint</strong> and <strong>outpaint</strong>.</p>
<h4 id="speech-to-text-whisper">Speech-To-Text (Whisper)</h4>
<p><code>Option -w</code> <strong>transcribes audio</strong> from
<em>mp3</em>, <em>mp4</em>, <em>mpeg</em>, <em>mpga</em>, <em>m4a</em>,
<em>wav</em>, <em>webm</em>, <em>flac</em> and <em>ogg</em> files. First
positional argument must be an <em>AUDIO</em> file. Optionally, set a
<em>TWO-LETTER</em> input language (<em>ISO-639-1</em>) as the second
argument. A PROMPT may also be set to guide the model’s style, or
continue a previous audio segment. The text prompt should match the
audio language.</p>
<p>Note that <code>option -w</code> can also be set to <strong>translate
audio</strong> input to any text language to the target language.</p>
<p><code>Option -W</code> <strong>translates audio</strong> stream to
<strong>English text</strong>. A PROMPT in English may be set to guide
the model as the second positional argument.</p>
<p>Set these options twice to have phrasal-level timestamps, options -ww
and -WW. Set thrice for word-level timestamps.</p>
<p>Combine <code>options -wW</code> <strong>with</strong>
<code>options -cc</code> to start <strong>chat with voice input</strong>
(Whisper) support. Additionally, set <code>option -z</code> to enable
<strong>text-to-speech</strong> (TTS) models and voice out.</p>
<h4 id="tts-text-to-voice">TTS (Text-To-Voice)</h4>
<p><code>Option -z</code> synthesises voice from text (TTS models). Set
a <em>voice</em> as the first positional parameter (“<em>alloy</em>”,
“<em>echo</em>”, “<em>fable</em>”, “<em>onyx</em>”, “<em>nova</em>”, or
“<em>shimmer</em>”). Set the second positional parameter as the
<em>voice speed</em> (<em>0.25</em> - <em>4.0</em>), and, finally the
<em>output file name</em> or the <em>format</em>, such as
“<em>./new_audio.mp3</em>” (“<em>mp3</em>”, “<em>opus</em>”,
“<em>aac</em>”, and “<em>flac</em>”), or “<em>-</em>” for stdout. Set
<code>options -vz</code> to <em>not</em> play received output.</p>
<h4 id="general-considerations">General Considerations</h4>
<p>User configuration is kept at “<em>~/.chatgpt.conf</em>”. Script
cache is kept at “<em>~/.cache/chatgptsh/</em>”.</p>
<p>The moderation endpoint can be accessed by setting the model name to
<em>omni-moderation-latest</em> (or
<em>text-moderation-latest</em>).</p>
<p>Stdin text is appended to PROMPT, to set a single PROMPT.</p>
<p>While <em>cURL</em> is in the middle of transmitting a request or
receiving a response, &lt;<em>CTRL-C</em>&gt; may be pressed once to
interrupt the call.</p>
<p>Press &lt;<em>CTRL-X</em> <em>CTRL-E</em>&gt; to edit command line in
text editor (readline).</p>
<p>Press &lt;<em>CTRL-J</em>&gt; or &lt;<em>CTRL-V</em>
<em>CTRL-J</em>&gt; for newline (readline).</p>
<p>Press &lt;<em>CTRL-\</em>&gt; to exit from the script (send
<em>QUIT</em> signal), or “<em>Q</em>” in user confirmation prompts.</p>
<p>A personal OpenAI API is required, set it with
<code>option --api-key</code>. See also <strong>ENVIRONMENT
section</strong>.</p>
<p>For complete model and settings information, refer to OpenAI API docs
at <a href="https://platform.openai.com/docs/"
class="uri">https://platform.openai.com/docs/</a>.</p>
<p>See the online man page and <code>chatgpt.sh</code> usage examples
at: <a href="https://gitlab.com/fenixdragao/shellchatgpt"
class="uri">https://gitlab.com/fenixdragao/shellchatgpt</a>.</p>
<h3 id="text-chat-completions">TEXT / CHAT COMPLETIONS</h3>
<h3 id="text-completion-modes-1">1. Text Completion Modes</h3>
<p>Given a prompt, the model will return one or more predicted
completions. For example, given a partial input, the language model will
try completing it until probable “<code>&lt;|endoftext|&gt;</code>”, or
other stop sequences (stops may be set with <code>-s</code>).</p>
<p><strong>Restart</strong> and <strong>start sequences</strong> may be
optionally set. Restart and start sequences are not set automatically if
the chat mode of text completions is not activated with
<code>option -c</code>.</p>
<p>Readline is set to work with <strong>multiline input</strong> and
pasting from the clipboard. Alternatively, set <code>option -u</code> to
enable pressing &lt;<em>CTRL-D</em>&gt; to flush input! Or set
<code>option -U</code> to set <em>cat command</em> as input
prompter.</p>
<!--  [DISABLED]
Type in a backslash "_\\_" as the last character of the input line
to append a literal newline once and return to edition,
or press \<_CTRL-V_ _CTRL-J_>.
-->
<p>Bash bracketed paste is enabled, meaning multiline input may be
pasted or typed, even without setting <code>options -uU</code>
(<em>v25.2+</em>).</p>
<p>Language model <strong>SKILLS</strong> can be activated with specific
prompts, see <a href="https://platform.openai.com/examples"
class="uri">https://platform.openai.com/examples</a>.</p>
<h3 id="chat-modes">2. Chat Modes</h3>
<h4 id="text-completions-chat">2.1 Text Completions Chat</h4>
<p>Set <code>option -c</code> to start chat mode of text completions. It
keeps a history file, and keeps new questions in context. This works
with a variety of models. Set <code>option -E</code> to exit on
response.</p>
<h4 id="native-chat-completions">2.2 Native Chat Completions</h4>
<p>Set the double <code>option -cc</code> to start chat completions
mode. More recent models are also the best option for many non-chat use
cases.</p>
<h4 id="q-a-format">2.3 Q &amp; A Format</h4>
<p>The defaults chat format is “<strong>Q &amp; A</strong>”. The
<strong>restart sequence</strong> “<em>\nQ: </em>” and the <strong>start
text</strong> “<em>\nA:</em>” are injected for the chat bot to work well
with text cmpls.</p>
<p>In native chat completions, setting a prompt with “<em>:</em>” as the
initial character sets the prompt as a <strong>SYSTEM</strong> message.
In text completions, however, typing a colon “<em>:</em>” at the start
of the prompt causes the text following it to be appended immediately to
the last (response) prompt text.</p>
<h4 id="voice-input-whisper-and-voice-output-tts">2.4 Voice input
(Whisper), and voice output (TTS)</h4>
<p>The <code>options -ccwz</code> may be combined to have voice
recording input and synthesised voice output, specially nice with chat
modes. When setting <code>flag -w</code> or <code>flag -z</code>, the
first positional parameters are read as Whisper or TTS arguments. When
setting both <code>flags -wz</code>, add a double hyphen to set first
Whisper, and then TTS arguments.</p>
<p>Set chat mode, plus Whisper language and prompt, and the TTS voice
option argument:</p>
<pre><code>chatgpt.sh -ccwz  en &#39;whisper prompt&#39;  --  nova</code></pre>
<h4 id="vision-and-multimodal-models">2.5 Vision and Multimodal
Models</h4>
<p>To send an <em>image</em> or <em>url</em> to <strong>vision
models</strong>, either set the image with the <code>!img</code> command
with one or more <em>filepaths</em> / <em>urls</em>.</p>
<pre><code>chatgpt.sh -cc -m gpt-4-vision-preview &#39;!img path/to/image.jpg&#39;</code></pre>
<p>Alternatively, set the <em>image paths</em> / <em>urls</em> at the
end of the text prompt interactively:</p>
<pre><code>chatgpt.sh -cc -m gpt-4-vision-preview

[...]
Q: In this first user prompt, what can you see?  https://i.imgur.com/wpXKyRo.jpeg</code></pre>
<h4 id="text-pdf-doc-and-url-dumps">2.6 Text, PDF, Doc, and URL
Dumps</h4>
<p>The user may add a <em>filepath</em> or <em>URL</em> to the end of
the prompt. The file is then read and the text content appended to the
user prompt. This is a basic text feature that works with any model.</p>
<pre><code>chatgpt.sh -cc

[...]
Q: What is this page: https://example.com

Q: Help me study this paper. ~/Downloads/Prigogine\ Perspective\ on\ Nature.pdf</code></pre>
<p>In the second example, the <em>PDF</em> will be dumped as text.</p>
<p>For PDF text dump support, <code>poppler/abiword</code> is required.
For <em>doc</em> and <em>odt</em> files, <code>LibreOffice</code> is
required. See the <strong>Optional Packages</strong> section.</p>
<p>Also note that <em>file paths</em> containing white spaces must be
<strong>blackslash-escaped</strong>.</p>
<h4 id="command-list">2.7 Command List</h4>
<p>While in chat mode, the following commands can be invoked in the new
prompt to execute a task or set parameters. The command operator may be
either “<code>!</code>” or “<code>/</code>”.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Misc</th>
<th style="text-align: left;">Commands</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>-S</code></td>
<td style="text-align: left;"><code>:</code>, <code>::</code>
[<em>PROMPT</em>]</td>
<td>Append user or system prompt to request buffer.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-S.</code></td>
<td style="text-align: left;"><code>-.</code> [<em>NAME</em>]</td>
<td>Load and edit custom prompt.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-S/</code></td>
<td style="text-align: left;"><code>-S%</code> [<em>NAME</em>]</td>
<td>Load and edit awesome prompt (zh).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-Z</code></td>
<td style="text-align: left;"><code>!last</code></td>
<td>Print last response JSON.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!#</code></td>
<td style="text-align: left;"><code>!save</code> [<em>PROMPT</em>]</td>
<td>Save current prompt to shell history. <em>‡</em></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!</code></td>
<td style="text-align: left;"><code>!r</code>, <code>!regen</code></td>
<td>Regenerate last response.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!!</code></td>
<td style="text-align: left;"><code>!rr</code></td>
<td>Regenerate response, edit prompt first.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!i</code></td>
<td style="text-align: left;"><code>!info</code></td>
<td>Information on model and session settings.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!j</code></td>
<td style="text-align: left;"><code>!jump</code></td>
<td>Jump to request, append start seq primer (text cmpls).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!!j</code></td>
<td style="text-align: left;"><code>!!jump</code></td>
<td>Jump to request, no response priming.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!cat</code></td>
<td style="text-align: left;">-</td>
<td>Cat prompter as one-shot, &lt;<em>CTRL-D</em>&gt; flush.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!cat</code></td>
<td style="text-align: left;"><code>!cat:</code>
[<em>TXT</em>|<em>URL</em>|<em>PDF</em>]</td>
<td>Cat <em>text</em>, <em>PDF</em> file, or dump <em>URL</em>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!dialog</code></td>
<td style="text-align: left;">-</td>
<td>Toggle the “dialog” interface.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!img</code></td>
<td style="text-align: left;"><code>!media</code>
[<em>FILE</em>|<em>URL</em>]</td>
<td>Append image, media, or URL to prompt.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!md</code></td>
<td style="text-align: left;"><code>!markdown</code>
[<em>SOFTW</em>]</td>
<td>Toggle markdown rendering in response.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!!md</code></td>
<td style="text-align: left;"><code>!!markdown</code>
[<em>SOFTW</em>]</td>
<td>Render last response in markdown.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!rep</code></td>
<td style="text-align: left;"><code>!replay</code></td>
<td>Replay last TTS audio response.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!res</code></td>
<td style="text-align: left;"><code>!resubmit</code></td>
<td>Resubmit last TTS recorded input from cache.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!p</code></td>
<td style="text-align: left;"><code>!pick</code>, [<em>PROPMT</em>]</td>
<td>File picker, appends filepath to user prompt. <em>‡</em></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!pdf</code></td>
<td style="text-align: left;"><code>!pdf:</code> [<em>FILE</em>]</td>
<td>Convert PDF and dump text.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!photo</code></td>
<td style="text-align: left;"><code>!!photo</code> [<em>INDEX</em>]</td>
<td>Take a photo, optionally set camera index (Termux). <em>‡</em></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!sh</code></td>
<td style="text-align: left;"><code>!shell</code> [<em>CMD</em>]</td>
<td>Run shell or <em>command</em>, and edit output. <em>‡</em></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!sh:</code></td>
<td style="text-align: left;"><code>!shell:</code> [<em>CMD</em>]</td>
<td>Same as <code>!sh</code> but apppend output as user.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!!sh</code></td>
<td style="text-align: left;"><code>!!shell</code> [<em>CMD</em>]</td>
<td>Run interactive shell (with <em>command</em>) and exit.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!url</code></td>
<td style="text-align: left;"><code>!url:</code> [<em>URL</em>]</td>
<td>Dump URL text or YouTube transcript text.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Script</th>
<th style="text-align: left;">Settings and UX</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>!fold</code></td>
<td style="text-align: left;"><code>!wrap</code></td>
<td>Toggle response wrapping.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-g</code></td>
<td style="text-align: left;"><code>!stream</code></td>
<td>Toggle response streaming.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-h</code></td>
<td style="text-align: left;"><code>!help</code> [<em>REGEX</em>]</td>
<td>Print help or grep help for regex.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-l</code></td>
<td style="text-align: left;"><code>!models</code> [<em>NAME</em>]</td>
<td>List language models or show model details.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-o</code></td>
<td style="text-align: left;"><code>!clip</code></td>
<td>Copy responses to clipboard.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-u</code></td>
<td style="text-align: left;"><code>!multi</code></td>
<td>Toggle multiline prompter. &lt;<em>CTRL-D</em>&gt; flush.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-uu</code></td>
<td style="text-align: left;"><code>!!multi</code></td>
<td>Multiline, one-shot. &lt;<em>CTRL-D</em>&gt; flush.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-U</code></td>
<td style="text-align: left;"><code>-UU</code></td>
<td>Toggle cat prompter or set one-shot. &lt;<em>CTRL-D</em>&gt;
flush.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-V</code></td>
<td style="text-align: left;"><code>!debug</code></td>
<td>Dump raw request block and confirm.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-v</code></td>
<td style="text-align: left;"><code>!ver</code></td>
<td>Toggle verbose modes.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-x</code></td>
<td style="text-align: left;"><code>!ed</code></td>
<td>Toggle text editor interface.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-xx</code></td>
<td style="text-align: left;"><code>!!ed</code></td>
<td>Single-shot text editor.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-y</code></td>
<td style="text-align: left;"><code>!tik</code></td>
<td>Toggle python tiktoken use.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!q</code></td>
<td style="text-align: left;"><code>!quit</code></td>
<td>Exit. Bye.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Settings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>-Nill</code></td>
<td style="text-align: left;"><code>!Nill</code></td>
<td>Toggle model max response (chat cmpls).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-M</code></td>
<td style="text-align: left;"><code>!NUM</code> <code>!max</code>
[<em>NUM</em>]</td>
<td>Maximum response tokens.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-N</code></td>
<td style="text-align: left;"><code>!modmax</code> [<em>NUM</em>]</td>
<td>Model token capacity.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-a</code></td>
<td style="text-align: left;"><code>!pre</code> [<em>VAL</em>]</td>
<td>Presence penalty.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-A</code></td>
<td style="text-align: left;"><code>!freq</code> [<em>VAL</em>]</td>
<td>Frequency penalty.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-b</code></td>
<td style="text-align: left;"><code>!best</code> [<em>NUM</em>]</td>
<td>Best-of n results.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-j</code></td>
<td style="text-align: left;"><code>!seed</code> [<em>NUM</em>]</td>
<td>Seed number (integer).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-K</code></td>
<td style="text-align: left;"><code>!topk</code> [<em>NUM</em>]</td>
<td>Top_k.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-m</code></td>
<td style="text-align: left;"><code>!mod</code> [<em>MOD</em>]</td>
<td>Model by name, empty to pick from list.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-n</code></td>
<td style="text-align: left;"><code>!results</code> [<em>NUM</em>]</td>
<td>Number of results.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-p</code></td>
<td style="text-align: left;"><code>!topp</code> [<em>VAL</em>]</td>
<td>Top_p.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-r</code></td>
<td style="text-align: left;"><code>!restart</code> [<em>SEQ</em>]</td>
<td>Restart sequence.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-R</code></td>
<td style="text-align: left;"><code>!start</code> [<em>SEQ</em>]</td>
<td>Start sequence.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-s</code></td>
<td style="text-align: left;"><code>!stop</code> [<em>SEQ</em>]</td>
<td>One stop sequence.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-t</code></td>
<td style="text-align: left;"><code>!temp</code> [<em>VAL</em>]</td>
<td>Temperature.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-w</code></td>
<td style="text-align: left;"><code>!rec</code> [<em>ARGS</em>]</td>
<td>Toggle Whisper. Optionally, set arguments.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-z</code></td>
<td style="text-align: left;"><code>!tts</code> [<em>ARGS</em>]</td>
<td>Toggle TTS chat mode (speech out).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!ka</code></td>
<td style="text-align: left;"><code>!keep-alive</code>
[<em>NUM</em>]</td>
<td>Set duration of model load in memory (Ollama).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!blk</code></td>
<td style="text-align: left;"><code>!block</code> [<em>ARGS</em>]</td>
<td>Set and add custom options to JSON request.</td>
</tr>
<tr class="even">
<td style="text-align: left;">-</td>
<td style="text-align: left;"><code>!multimodal</code></td>
<td>Toggle model as multimodal.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Session</th>
<th style="text-align: left;">Management</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>-H</code></td>
<td style="text-align: left;"><code>!hist</code></td>
<td>Edit history in editor.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>-P</code></td>
<td style="text-align: left;"><code>-HH</code>, <code>!print</code></td>
<td>Print session history.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>-L</code></td>
<td style="text-align: left;"><code>!log</code> [<em>FILEPATH</em>]</td>
<td>Save to log file.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!br</code></td>
<td style="text-align: left;"><code>!break</code>,
<code>!new</code></td>
<td>Start new session (session break).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!ls</code></td>
<td style="text-align: left;"><code>!list</code> [<em>GLOB</em>]</td>
<td>List History files with <em>glob</em> in <em>name</em>. Intruction
prompts: “<em>pr</em>”. Awesome: “<em>awe</em>”. All: “<em>.</em>”.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!grep</code></td>
<td style="text-align: left;"><code>!sub</code> [<em>REGEX</em>]</td>
<td>Search sessions (for regex) and copy session to hist tail.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!c</code></td>
<td style="text-align: left;"><code>!copy</code> [<em>SRC_HIST</em>]
[<em>DEST_HIST</em>]</td>
<td>Copy session from source to destination.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!f</code></td>
<td style="text-align: left;"><code>!fork</code>
[<em>DEST_HIST</em>]</td>
<td>Fork current session to destination.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!k</code></td>
<td style="text-align: left;"><code>!kill</code> [<em>NUM</em>]</td>
<td>Comment out <em>n</em> last entries in history file.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!!k</code></td>
<td style="text-align: left;"><code>!!kill</code>
[[<em>0</em>]<em>NUM</em>]</td>
<td>Dry-run of command <code>!kill</code>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>!s</code></td>
<td style="text-align: left;"><code>!session</code>
[<em>HIST_FILE</em>]</td>
<td>Change to, search for, or create history file.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>!!s</code></td>
<td style="text-align: left;"><code>!!session</code>
[<em>HIST_FILE</em>]</td>
<td>Same as <code>!session</code>, break session.</td>
</tr>
</tbody>
</table>
<div class="line-block"><em>:</em> Commands with a <em>colon</em> have
their output appended to the prompt.</div>
<div class="line-block"><em>‡</em> Commands with <em>double dagger</em>
may be invoked at the very end of the input prompt.</div>
<div class="line-block">E.g.: “<code>/temp</code> <em>0.7</em>”,
“<code>!mod</code><em>gpt-4</em>”, “<code>-p</code> <em>0.2</em>”,
“<code>/session</code> <em>HIST_NAME</em>”, “[<em>PROMPT</em>]
<code>/pick</code>”, and “[<em>PROMPT</em>] <code>/sh</code>”.</div>
<p>The “<code>/pick</code>” command opens a file picker (usually a
command-line file manager). The selected file’s path will be appended to
the current prompt in editing mode.</p>
<p>The “<code>/pick</code>” and “<code>/sh</code>” commands may be run
when typed at the end of the current prompt, such as “[<em>PROMPT</em>]
<code>/sh</code>”, which opens a new shell instance to execute commands
interactively. The output of these commands is appended to the current
prompt.</p>
<p>When the “<code>/pick</code>” command is run at the end of the
prompt, the selected file path is appended instead.</p>
<p>Command “<code>!block</code> [<em>ARGS</em>]” may be run to set raw
model options in JSON syntax according to each API. Alternatively, set
envar <strong>$BLOCK_USR</strong>.</p>
<h4 id="session-management">2.8 Session Management</h4>
<p>The script uses a <em>TSV file</em> to record entries, which is kept
at the script cache directory. A new history file can be created or an
existing one changed to with command “<code>/session</code>
[<em>HIST_FILE</em>]”, in which <em>HIST_FILE</em> is the file name of
(with or without the <em>.tsv</em> extension), or path to, a history
file.</p>
<p>When the first positional argument to the script is the command
operator forward slash followed by a history file name, the command
<code>/session</code> is assumed.</p>
<p>A history file can contain many sessions. The last one (the tail
session) is always loaded if the resume <code>option -C</code> is
set.</p>
<h5 id="copying-and-resuming-older-sessions">Copying and resuming older
sessions</h5>
<p>To continue from an old session, either <strong>/sub</strong> or
<strong>/fork.</strong> it as the current session. The shorthand for
this feature is <strong>/.</strong>.</p>
<p>It is also possible to <code>/grep [regex]</code> for a session. This
will fork the selected session and resume it.</p>
<p>If “<code>/copy</code> <em>current</em>” is run, a selector is shown
to choose and copy a session to the tail of the current history file,
and resume it. This is equivalent to running “<code>/fork</code>”.</p>
<p>It is also possible to copy sessions of a history file to another
file when a second argument is given to the command with the history
file name, such as “<code>/copy</code> [<em>SRC_HIST_FILE</em>]
[<em>DEST_HIST_FILE</em>]”, and a dot as file name means the current
history file.</p>
<h5 id="changing-session">Changing session</h5>
<p>To load an older session from a history file that is different from
the defaults, there are some options.</p>
<p>Change to it with command <code>!session [name]</code>, and then
<code>!fork</code> the older session to the active session.</p>
<p>Or, <code>!copy [orign] [dest]</code> the session from a history file
to the current or other history file.</p>
<p>In these cases, a pickup interface should open to let the user choose
the correct session from the history file.</p>
<h5 id="history-file-editing">History file editing</h5>
<p>To change the chat context at run time, the history file may be
edited with the “<code>/hist</code>” command (also for context
injection). Delete history entries or comment them out with
“<code>#</code>”.</p>
<h4 id="completion-preview-regeneration">2.9 Completion Preview /
Regeneration</h4>
<p>To preview a prompt completion before committing it to history,
append a forward slash “<code>/</code>” to the prompt as the last
character. Regenerate it again or flush/accept the prompt and
response.</p>
<p>After a response has been written to the history file,
<strong>regenerate</strong> it with command “<code>!regen</code>” or
type in a single exclamation mark or forward slash “<code>/</code>” in
the new empty prompt (twice “<code>//</code>” for editing the prompt
before new request).</p>
<h3 id="prompt-engineering-and-design">3. Prompt Engineering and
Design</h3>
<p>Minimal <strong>INSTRUCTION</strong> to behave like a chatbot is
given with chat <code>options -cc</code>, unless otherwise explicitly
set by the user.</p>
<p>On chat mode, if no INSTRUCTION is set, minimal instruction is given,
and some options auto set, such as increasing temp and presence penalty,
in order to un-lobotomise the bot. With cheap and fast models of text
cmpls, such as Curie, the <code>best_of</code> option may be worth
setting (to 2 or 3).</p>
<p>Prompt engineering is an art on itself. Study carefully how to craft
the best prompts to get the most out of text, code and chat cmpls
models.</p>
<p>Certain prompts may return empty responses. Maybe the model has
nothing to further complete input or it expects more text. Try trimming
spaces, appending a full stop/ellipsis, resetting temperature, or adding
more text.</p>
<p>Prompts ending with a space character may result in lower quality
output. This is because the API already incorporates trailing spaces in
its dictionary of tokens.</p>
<p>Note that the model’s steering and capabilities require prompt
engineering to even know that it should answer the questions.</p>
<p>It is also worth trying to sample 3 - 5 times (increasing the number
of responses with option <code>-n 3</code>, for example) in order to
obtain a good response.</p>
<p>For more on prompt design, see:</p>
<ul>
<li><a
href="https://platform.openai.com/docs/guides/completion/prompt-design"
class="uri">https://platform.openai.com/docs/guides/completion/prompt-design</a></li>
<li><a
href="https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md"
class="uri">https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md</a></li>
</ul>
<p>See detailed info on settings for each endpoint at:</p>
<ul>
<li><a href="https://platform.openai.com/docs/"
class="uri">https://platform.openai.com/docs/</a></li>
</ul>
<!--
### CODE COMPLETIONS _(discontinued)_

Codex models are discontinued. Use davinci or _gpt-3.5+ models_ for coding tasks.

-- 
Turn comments into code, complete the next line or function in
context, add code comments, and rewrite code for efficiency,
amongst other functions.
--

Start with a comment with instructions, data or code. To create
useful completions it's helpful to think about what information
a programmer would need to perform a task. 
-->
<!--
### TEXT EDITS  _(discontinued)_

--
This endpoint is set with models with **edit** in their name or
`option -e`. Editing works by setting INSTRUCTION on how to modify
a prompt and the prompt proper.

The edits endpoint can be used to change the tone or structure
of text, or make targeted changes like fixing spelling. Edits
work well on empty prompts, thus enabling text generation similar
to the completions endpoint. 

Alternatively,
--

Use _gpt-4+ models_ and the right instructions.
-->
<h3 id="escaping-new-lines-and-tabs">ESCAPING NEW LINES AND TABS</h3>
<p>Input sequences “<em>\n</em>” and “<em>\t</em>” are only treated
specially in restart, start and stop sequences!</p>
<h3 id="custom-awesome-prompts">CUSTOM / AWESOME PROMPTS</h3>
<p>When the argument to <code>option -S</code> starts with a full stop,
such as “<code>-S</code> <code>.</code><em>my_prompt</em>”, load, search
for, or create <em>my_prompt</em> prompt file. If two full stops are
prepended to the prompt name, load it silently. If a comma is used
instead, such as “<code>-S</code> <code>,</code><em>my_prompt</em>”,
edit the prompt file, and then load it.</p>
<p>When the argument to <code>option -S</code> starts with a backslash
or a percent sign, such as “<code>-S</code>
<code>/</code><em>linux_terminal</em>”, search for an
<em>awesome-chatgpt-prompt(-zh)</em> (by Fatih KA and PlexPt). Set
“<code>//</code>” or “<code>%%</code>” to refresh local cache. Use with
<em>davinci</em> and <em>gpt-3.5+</em> models.</p>
<p>These options also set corresponding history files automatically.</p>
<p>Please note and make sure to backup your important custom prompts!
They are located at “<code>~/.cache/chatgptsh/</code>” with the
extension “<em>.pr</em>”.</p>
<h3 id="images-dall-e">IMAGES / DALL-E</h3>
<h3 id="image-generations">1. Image Generations</h3>
<p>An image can be created given a text prompt. A text PROMPT of the
desired image(s) is required. The maximum length is 1000 characters.</p>
<h3 id="image-variations">2. Image Variations</h3>
<p>Variations of a given <em>IMAGE</em> can be generated. The
<em>IMAGE</em> to use as the basis for the variations must be a valid
PNG file, less than 4MB and square.</p>
<h3 id="image-edits">3. Image Edits</h3>
<p>To edit an <em>IMAGE</em>, a <em>MASK</em> file may be optionally
provided. If <em>MASK</em> is not provided, <em>IMAGE</em> must have
transparency, which will be used as the mask. A text prompt is
required.</p>
<h4 id="imagemagick">3.1 ImageMagick</h4>
<p>If <strong>ImageMagick</strong> is available, input <em>IMAGE</em>
and <em>MASK</em> will be checked and processed to fit dimensions and
other requirements.</p>
<h4 id="transparent-colour-and-fuzz">3.2 Transparent Colour and
Fuzz</h4>
<p>A transparent colour must be set with
“<code>-@</code>[<em>COLOUR</em>]” to create the mask.
Defaults=<em>black</em>.</p>
<p>By defaults, the <em>COLOUR</em> must be exact. Use the
<code>fuzz option</code> to match colours that are close to the target
colour. This can be set with “<code>-@</code>[<em>VALUE%</em>]” as a
percentage of the maximum possible intensity, for example
“<code>-@</code><em>10%black</em>”.</p>
<p>See also:</p>
<ul>
<li><a href="https://imagemagick.org/script/color.php"
class="uri">https://imagemagick.org/script/color.php</a></li>
<li><a
href="https://imagemagick.org/script/command-line-options.php#fuzz"
class="uri">https://imagemagick.org/script/command-line-options.php#fuzz</a></li>
</ul>
<h4 id="mask-file-alpha-channel">3.3 Mask File / Alpha Channel</h4>
<p>An alpha channel is generated with <strong>ImageMagick</strong> from
any image with the set transparent colour (defaults to <em>black</em>).
In this way, it is easy to make a mask with any black and white image as
a template.</p>
<h4 id="in-paint-and-out-paint">3.4 In-Paint and Out-Paint</h4>
<p>In-painting is achieved setting an image with a MASK and a
prompt.</p>
<p>Out-painting can also be achieved manually with the aid of this
script. Paint a portion of the outer area of an image with
<em>alpha</em>, or a defined <em>transparent</em> <em>colour</em> which
will be used as the mask, and set the same <em>colour</em> in the script
with <code>-@</code>. Choose the best result amongst many results to
continue the out-painting process step-wise.</p>
<h3 id="audio-whisper">AUDIO / WHISPER</h3>
<h3 id="transcriptions">1. Transcriptions</h3>
<p>Transcribes audio file or voice record into the set language. Set a
<em>two-letter</em> <em>ISO-639-1</em> language code (<em>en</em>,
<em>es</em>, <em>ja</em>, or <em>zh</em>) as the positional argument
following the input audio file. A prompt may also be set as last
positional parameter to help guide the model. This prompt should match
the audio language.</p>
<p>If the last positional argument is “.” or “last” exactly, it will
resubmit the last recorded audio input file from cache.</p>
<p>Note that if the audio language is different from the set language
code, output will be on the language code (translation).</p>
<h3 id="translations">2. Translations</h3>
<p>Translates audio into <strong>English</strong>. An optional text to
guide the model’s style or continue a previous audio segment is optional
as last positional argument. This prompt should be in English.</p>
<p>Setting <strong>temperature</strong> has an effect, the higher the
more random.</p>
<h3 id="provider-integrations">Provider Integrations</h3>
<p>For LocalAI integration, run the script with
<code>option --localai</code>, or set environment
<strong>$OPENAI_BASE_URL</strong> with the server Base URL.</p>
<p>For Mistral AI set environment variable
<strong>$MISTRAL_API_KEY</strong>, and run the script with
<code>option --mistral</code> or set <strong>$OPENAI_BASE_URL</strong>
to “https://api.mistral.ai/”. Prefer setting command line
<code>option --mistral</code> for complete integration.
<!-- also see: \$MISTRAL_BASE_URL --></p>
<p>For Ollama, set <code>option -O</code> (<code>--ollama</code>), and
set <strong>$OLLAMA_BASE_URL</strong> if the server URL is different
from the defaults.</p>
<p>Note that model management (downloading and setting up) must follow
the Ollama project guidelines and own methods.</p>
<p>For Google Gemini, set environment variable
<strong>$GOOGLE_API_KEY</strong>, and run the script with the command
line <code>option --google</code>.</p>
<p>For Groq, set the environmental variable <code>$GROQ_API_KEY</code>.
Run the script with <code>option --groq</code>. Whisper endpoint
available.</p>
<p>For Anthropic, set envar <code>$ANTHROPIC_API_KEY</code> and run the
script with command line <code>option --anthropic</code>.</p>
<p>And for GitHub Models, <code>$GITHUB_TOKEN</code> and invoke the
script with <code>option --github</code>.</p>
<h3 id="environment">ENVIRONMENT</h3>
<p><strong>BLOCK_USR</strong></p>
<dl>
<dt><strong>BLOCK_USR_TTS</strong></dt>
<dd>
<p>Extra options for the request JSON block (e.g. “<em>"seed": 33,
"dimensions": 1024</em>”).</p>
</dd>
<dt><strong>CACHEDIR</strong></dt>
<dd>
<p>Script cache directory base.</p>
</dd>
<dt><strong>CHATGPTRC</strong></dt>
<dd>
<p>Path to the user <em>configuration file</em>.</p>
<p>Defaults="<em>~/.chatgpt.conf</em>"</p>
</dd>
<dt><strong>FILECHAT</strong></dt>
<dd>
<p>Path to a history / session TSV file (script-formatted).</p>
</dd>
<dt><strong>INSTRUCTION</strong></dt>
<dd>
<p>Initial initial instruction or system message.</p>
</dd>
<dt><strong>INSTRUCTION_CHAT</strong></dt>
<dd>
<p>Initial initial instruction or system message for chat mode.</p>
</dd>
</dl>
<p><strong>MOD_CHAT</strong>, <strong>MOD_IMAGE</strong>,
<strong>MOD_AUDIO</strong>,</p>
<p><strong>MOD_SPEECH</strong>, <strong>MOD_LOCALAI</strong>,
<strong>MOD_OLLAMA</strong>,</p>
<p><strong>MOD_MISTRAL</strong>, <strong>MOD_GOOGLE</strong>,
<strong>MOD_GROQ</strong>,</p>
<dl>
<dt><strong>MOD_AUDIO_GROQ</strong>, <strong>MOD_ANTHROPIC</strong>,
<strong>MOD_GITHUB</strong></dt>
<dd>
<p>Set default model for each endpoint / provider.</p>
</dd>
</dl>
<p><strong>OPENAI_BASE_URL</strong></p>
<dl>
<dt><strong>OPENAI_URL_PATH</strong></dt>
<dd>
<p>Main Base URL setting. Alternatively, provide a <em>URL_PATH</em>
parameter with the full url path to disable endpoint auto selection.</p>
</dd>
<dt><strong>PROVIDER_BASE_URL</strong></dt>
<dd>
<p>Base URLs for each service provider: <em>LOCALAI</em>,
<em>OLLAMA</em>, <em>MISTRAL</em>, <em>GOOGLE</em>, <em>ANTHROPIC</em>,
<em>GROQ</em>, and <em>GITHUB</em>.</p>
</dd>
</dl>
<p><strong>OPENAI_API_KEY</strong></p>
<dl>
<dt><strong>PROVIDER_API_KEY</strong></dt>
<dd>
<p>Keys for OpenAI, GoogleAI, MistralAI, Groq, and GitHub APIs.</p>
</dd>
<dt><strong>OUTDIR</strong></dt>
<dd>
<p>Output directory for received image and audio.</p>
</dd>
</dl>
<p><strong>RESTART</strong></p>
<dl>
<dt><strong>START</strong></dt>
<dd>
<p>Restart and start sequences. May be set to <em>null</em>.</p>
<p>Restart=“<em>\nQ: </em>” Start="<em>\nA:</em>" (chat mode)</p>
</dd>
</dl>
<p><strong>VISUAL</strong></p>
<dl>
<dt><strong>EDITOR</strong></dt>
<dd>
<p>Text editor for external prompt editing.</p>
<p>Defaults="<em>vim</em>"</p>
</dd>
<dt><strong>CLIP_CMD</strong></dt>
<dd>
<p>Clipboard set command, e.g. “<em>xsel</em> <em>-b</em>”,
“<em>pbcopy</em>”.</p>
</dd>
<dt><strong>PLAY_CMD</strong></dt>
<dd>
<p>Audio player command, e.g. “<em>mpv –no-video –vo=null</em>”.</p>
</dd>
<dt><strong>REC_CMD</strong></dt>
<dd>
<p>Audio recorder command, e.g. “<em>sox -d</em>”.</p>
</dd>
</dl>
<h3 id="colour-themes">COLOUR THEMES</h3>
<p>The colour scheme may be customised. A few themes are available in
the template configuration file.</p>
<p>A small colour library is available for the user conf file to
personalise the theme colours.</p>
<p>The colour palette is composed of <em>$Red</em>, <em>$Green</em>,
<em>$Yellow</em>, <em>$Blue</em>, <em>$Purple</em>, <em>$Cyan</em>,
<em>$White</em>, <em>$Inv</em> (invert), and <em>$Nc</em> (reset)
variables.</p>
<p>Bold variations are defined as <em>$BRed</em>, <em>$BGreen</em>, etc,
and background colours can be set with <em>$On_Yellow</em>,
<em>$On_Blue</em>, etc.</p>
<p>Alternatively, raw escaped color sequences, such as
<em>\e[0;35m</em>, and <em>\e[1;36m</em> may be set.</p>
<p>Theme colours are named variables from <code>Colour1</code> to about
<code>Colour11</code>, and may be set with colour-named variables or raw
escape sequences (these must not change cursor position).</p>
<h3 id="required-packages">REQUIRED PACKAGES</h3>
<ul>
<li><code>Bash</code></li>
<li><code>cURL</code>, and <code>JQ</code></li>
</ul>
<h3 id="optional-packages">OPTIONAL PACKAGES</h3>
<p>Optional packages for specific features.</p>
<ul>
<li><code>Base64</code> - Image endpoint, vision models</li>
<li><code>Python</code> - Modules tiktoken, markdown, bs4</li>
<li><code>ImageMagick</code>/<code>fbida</code> - Image edits and
variations</li>
<li><code>SoX</code>/<code>Arecord</code>/<code>FFmpeg</code> - Record
input (Whisper)</li>
<li><code>mpv</code>/<code>SoX</code>/<code>Vlc</code>/<code>FFplay</code>/<code>afplay</code>
- Play TTS output</li>
<li><code>xdg-open</code>/<code>open</code>/<code>xsel</code>/<code>xclip</code>/<code>pbcopy</code>
- Open images, set clipboard</li>
<li><code>W3M</code>/<code>Lynx</code>/<code>ELinks</code>/<code>Links</code>
- Dump URL text</li>
<li><code>bat</code>/<code>Pygmentize</code>/<code>Glow</code>/<code>mdcat</code>/<code>mdless</code>
- Markdown support</li>
<li><code>termux-api</code>/<code>termux-tools</code>/<code>play-audio</code>
- Termux system</li>
<li><code>poppler</code>/<code>gs</code>/<code>abiword</code>/<code>ebook-convert</code>/<code>LibreOffice</code>
- Dump PDF or Doc as text</li>
<li><code>dialog</code>/<code>kdialog</code>/<code>zenity</code>/<code>osascript</code>/<code>termux-dialog</code>
- File picker</li>
</ul>
<h3 id="bugs">BUGS</h3>
<p>Bash “read command” may not correctly display input buffers larger
than the TTY screen size during editing. However, input buffers remain
unaffected. Use the text editor interface for big prompt editing.</p>
<p>File paths containing spaces may not work correctly with some script
features.</p>
<p>Bash truncates input on “\000” (null).</p>
<p>The script logic resembles a bowl of spaghetti code after a cat
fight.</p>
<p>Garbage in, garbage out. An idiot savant.</p>
<!-- NOT ANYMORE
Input sequences _\\n_, and _\\t_ must be double escaped to be treated
literally, otherwise these will be interpreted as escaped newlines,
and horizontal tabs in JSON encoding. This is specially important when
input contains *software code*. -->
<!-- Changing models in the same session may generate token count errors
because the recorded token count may differ from model encoding to encoding.
Set `option -y` for accurate token counting. -->
<!-- With the exception of Davinci and newer base models, older models were designed
to be run as one-shot. -->
<!-- The script is expected to work with language models and inputs
up to 32k tokens. -->
<!-- OBVIOUSLY, ALREADY MENTIONED
Instruction prompts are required for the model to even know that
it should answer questions. -->
<!--
`Zsh` does not read history file in non-interactive mode.

`Ksh93` mangles multibyte characters when re-editing input prompt
and truncates input longer than 80 chars. Workaround is to move
cursor one char and press the up arrow key.

`Ksh2020` lacks functionality compared to `Ksh83u+`, such as `read`
with history, so avoid it.
-->
<h3 id="limits">LIMITS</h3>
<p>The script objective is to implement most features of OpenAI API
version 1 but not all endpoints or options will be covered.</p>
<p>This project <em>doesn’t</em> support “Function Calling” or
“Structured Outputs”.</p>
<h3 id="options">OPTIONS</h3>
<h4 id="service-providers">Service Providers</h4>
<dl>
<dt><strong>--anthropic</strong>, <strong>--ant</strong></dt>
<dd>
<p>Set Anthropic integration (cmpls/chat).</p>
</dd>
<dt><strong>--github</strong>, <strong>--git</strong></dt>
<dd>
<p>Set GitHub Models integration (chat).</p>
</dd>
<dt><strong>--google</strong>, <strong>-goo</strong></dt>
<dd>
<p>Set Google Gemini integration (cmpls/chat).</p>
</dd>
<dt><strong>--groq</strong></dt>
<dd>
<p>Set Groq integration (chat).</p>
</dd>
<dt><strong>--localai</strong></dt>
<dd>
<p>Set LocalAI integration (cmpls/chat).</p>
</dd>
<dt><strong>--mistral</strong></dt>
<dd>
<p>Set Mistral AI integration (chat).</p>
</dd>
<dt><strong>--openai</strong></dt>
<dd>
<p>Reset service integrations.</p>
</dd>
<dt><strong>-O</strong>, <strong>--ollama</strong></dt>
<dd>
<p>Set and make requests to Ollama server (cmpls/chat).</p>
</dd>
</dl>
<h4 id="configuration-file">Configuration File</h4>
<dl>
<dt><strong>-f</strong>, <strong>--no-conf</strong></dt>
<dd>
<p>Ignore user configuration file.</p>
</dd>
<dt><strong>-F</strong></dt>
<dd>
<p>Edit configuration file with text editor, if it exists.</p>
<p>$CHATGPTRC="<em>~/.chatgpt.conf</em>".</p>
</dd>
<dt><strong>-FF</strong></dt>
<dd>
<p>Dump template configuration file to stdout.</p>
</dd>
</dl>
<h4 id="sessions-and-history-files">Sessions and History Files</h4>
<dl>
<dt><strong>-H</strong>, <strong>--hist</strong>
[<code>/</code><em>HIST_FILE</em>]</dt>
<dd>
<p>Edit history file with text editor or pipe to stdout.</p>
<p>A history file name can be optionally set as argument.</p>
</dd>
<dt><strong>-P</strong>, <strong>-PP</strong>, <strong>--print</strong>
[<code>/</code><em>HIST_FILE</em>]</dt>
<dd>
<p>Print out last history session.</p>
<p>Set twice to print commented out history entries, inclusive. Heeds
<code>options -ccdrR</code>.</p>
<p>These are aliases to <strong>-HH</strong> and <strong>-HHH</strong>,
respectively.</p>
</dd>
</dl>
<h4 id="input-modes">Input Modes</h4>
<dl>
<dt><strong>-u</strong>, <strong>--multiline</strong></dt>
<dd>
<p>Toggle multiline prompter, &lt;<em>CTRL-D</em>&gt; flush.</p>
</dd>
<dt><strong>-U</strong>, <strong>--cat</strong></dt>
<dd>
<p>Set cat prompter, &lt;<em>CTRL-D</em>&gt; flush.</p>
</dd>
<dt><strong>-x</strong>, <strong>-xx</strong>,
<strong>--editor</strong></dt>
<dd>
<p>Edit prompt in text editor.</p>
<p>Set twice to run the text editor interface a single time for the
first user input.</p>
<p>Set <code>options -eex</code> to edit last buffer from cache.</p>
</dd>
</dl>
<h4 id="interface-modes">Interface Modes</h4>
<dl>
<dt><strong>-c</strong>, <strong>--chat</strong></dt>
<dd>
<p>Chat mode in text completions (used with
<code>options -wzvv</code>).</p>
</dd>
<dt><strong>-cc</strong></dt>
<dd>
<p>Chat mode in chat completions (used with
<code>options -wzvv</code>).</p>
</dd>
<dt><strong>-C</strong>, <strong>--continue</strong>,
<strong>--resume</strong></dt>
<dd>
<p>Continue from (resume) last session (cmpls/chat).</p>
</dd>
<dt><strong>-d</strong>, <strong>--text</strong></dt>
<dd>
<p>Start new multi-turn session in plain text completions.</p>
</dd>
<dt><strong>-e</strong>, <strong>--edit</strong></dt>
<dd>
<p>Edit first input from stdin or file (cmpls/chat).</p>
<p>With <code>options -eex</code>, edit last text editor buffer from
cache.</p>
</dd>
<dt><strong>-E</strong>, <strong>-EE</strong>,
<strong>--exit</strong></dt>
<dd>
<p>Exit on first run (even with options -cc).</p>
</dd>
<dt><strong>-g</strong>, <strong>--stream</strong>
(<em>defaults</em>)</dt>
<dd>
<p>Set response streaming.</p>
</dd>
<dt><strong>-G</strong>, <strong>--no-stream</strong></dt>
<dd>
<p>Unset response streaming.</p>
</dd>
<dt><strong>-i</strong>, <strong>--image</strong> [<em>PROMPT</em>]</dt>
<dd>
<p>Generate images given a prompt. Set <em>option -v</em> to not open
response.</p>
</dd>
<dt><strong>-i</strong> [<em>PNG</em>]</dt>
<dd>
<p>Create variations of a given image.</p>
</dd>
<dt><strong>-i</strong> [<em>PNG</em>] [<em>MASK</em>]
[<em>PROMPT</em>]</dt>
<dd>
<p>Edit image with mask and prompt (required).</p>
</dd>
<dt><strong>-q</strong>, <strong>-qq</strong>,
<strong>--insert</strong></dt>
<dd>
<p>Insert text rather than completing only. May be set twice for
multi-turn.</p>
<p>Use “<em>[insert]</em>” to indicate where the language model should
insert text (<code>instruct</code> and Mistral <code>code</code>
models).</p>
</dd>
</dl>
<p><strong>-S</strong> <strong>.</strong>[<em>PROMPT_NAME</em>],
<strong>-.</strong>[<em>PROMPT_NAME</em>]</p>
<dl>
<dt><strong>-S</strong> <strong>,</strong>[<em>PROMPT_NAME</em>],
<strong>-,</strong>[<em>PROMPT_NAME</em>]</dt>
<dd>
<p>Load, search for, or create custom prompt.</p>
<p>Set <code>.</code>[<em>PROMPT</em>] to load prompt silently.</p>
<p>Set <code>,</code>[<em>PROMPT</em>] to single-shot edit prompt.</p>
<p>Set <code>,,</code>[<em>PROMPT</em>] to edit the prompt template
file.</p>
<p>Set <code>.</code><em>?</em>, or <code>.</code><em>list</em> to list
all prompt files.</p>
</dd>
</dl>
<p><strong>-S</strong>
<strong>/</strong>[<em>AWESOME_PROMPT_NAME</em>]</p>
<dl>
<dt><strong>-S</strong>
<strong>%</strong>[<em>AWESOME_PROMPT_NAME_ZH</em>]</dt>
<dd>
<p>Set or search for an <em>awesome-chatgpt-prompt(-zh)</em>.
<em>Davinci</em> and <em>gpt3.5+</em> models.</p>
<p>Set <strong>//</strong> or <strong>%%</strong> instead to refresh
cache.</p>
</dd>
</dl>
<p><strong>-T</strong>, <strong>--tiktoken</strong></p>
<dl>
<dt><strong>-TT</strong>, <strong>-TTT</strong></dt>
<dd>
<p>Count input tokens with python Tiktoken (ignores special tokens).</p>
<p>Set twice to print tokens, thrice to available encodings.</p>
<p>Set the model or encoding with <code>option -m</code>.</p>
<p>It heeds <code>options -ccm</code>.</p>
</dd>
<dt><strong>-w</strong>, <strong>--transcribe</strong> [<em>AUD</em>]
[<em>LANG</em>] [<em>PROMPT</em>]</dt>
<dd>
<p>Transcribe audio file into text. LANG is optional. A prompt that
matches the audio language is optional. Audio will be transcribed or
translated to the target LANG.</p>
<p>Set twice to phrase or thrice for word-level timestamps (-www).</p>
<p>With <code>options -vv</code>, stop voice recorder on silence auto
detection.</p>
</dd>
<dt><strong>-W</strong>, <strong>--translate</strong> [<em>AUD</em>]
[<em>PROMPT-EN</em>]</dt>
<dd>
<p>Translate audio file into English text.</p>
<p>Set twice to phrase or thrice for word-level timestamps (-WWW).</p>
</dd>
<dt><strong>-z</strong>, <strong>--tts</strong>
[<em>OUTFILE</em>|<em>FORMAT</em>|<em>-</em>] [<em>VOICE</em>]
[<em>SPEED</em>] [<em>PROMPT</em>]</dt>
<dd>
<p>Synthesise speech from text prompt. Takes a voice name, speed and
text prompt.</p>
<p>Set <code>option -v</code> to not play response automatically.</p>
</dd>
</dl>
<h4 id="model-settings">Model Settings</h4>
<dl>
<dt><strong>-@</strong>, <strong>--alpha</strong>
[[<em>VAL%</em>]<em>COLOUR</em>]</dt>
<dd>
<p>Set transparent colour of image mask. Def=<em>black</em>.</p>
<p>Fuzz intensity can be set with [<em>VAL%</em>]. Def=<em>0%</em>.</p>
</dd>
<dt><strong>-Nill</strong></dt>
<dd>
<p>Unset model max response (chat cmpls only).</p>
</dd>
</dl>
<p><strong>-NUM</strong></p>
<dl>
<dt><strong>-M</strong>, <strong>--max</strong>
[<em>NUM</em>[<em>-NUM</em>]]</dt>
<dd>
<p>Set maximum number of <em>response tokens</em>.
Def=<em>1024</em>.</p>
<p>A second number in the argument sets model capacity.</p>
</dd>
<dt><strong>-N</strong>, <strong>--modmax</strong> [<em>NUM</em>]</dt>
<dd>
<p>Set <em>model capacity</em> tokens. Def=<em>auto</em>,
Fallback=<em>4000</em>.</p>
</dd>
<dt><strong>-a</strong>, <strong>--presence-penalty</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Set presence penalty (cmpls/chat, -2.0 - 2.0).</p>
</dd>
<dt><strong>-A</strong>, <strong>--frequency-penalty</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Set frequency penalty (cmpls/chat, -2.0 - 2.0).</p>
</dd>
<dt><strong>-b</strong>, <strong>--best-of</strong> [<em>NUM</em>]</dt>
<dd>
<p>Set best of, must be greater than <code>option -n</code> (cmpls).
Def=<em>1</em>.</p>
</dd>
<dt><strong>-B</strong>, <strong>--logprobs</strong> [<em>NUM</em>]</dt>
<dd>
<p>Request log probabilities, also see -Z (cmpls, 0 - 5),</p>
</dd>
<dt><strong>-j</strong>, <strong>–seed</strong> [<em>NUM</em>]</dt>
<dd>
<p>Set a seed for deterministic sampling (integer).</p>
</dd>
<dt><strong>-K</strong>, <strong>–top-k</strong> [<em>NUM</em>]</dt>
<dd>
<p>Set Top_k value (local-ai, ollama, google).</p>
</dd>
<dt><strong>--keep-alive</strong>,
<strong>--ka</strong>=[<em>NUM</em>]</dt>
<dd>
<p>Set how long the model will stay loaded into memory (ollama).</p>
</dd>
<dt><strong>-m</strong>, <strong>--model</strong> [<em>MODEL</em>]</dt>
<dd>
<p>Set language <em>MODEL</em> name.
Def=<em>gpt-3.5-turbo-instruct</em>/<em>gpt-4o</em>.</p>
<p>Set <em>MODEL</em> name as “<em>.</em>” to pick from the list.</p>
</dd>
<dt><strong>--multimodal</strong></dt>
<dd>
<p>Set model as multimodal.</p>
</dd>
<dt><strong>-n</strong>, <strong>--results</strong> [<em>NUM</em>]</dt>
<dd>
<p>Set number of results. Def=<em>1</em>.</p>
</dd>
<dt><strong>-p</strong>, <strong>--top-p</strong> [<em>VAL</em>]</dt>
<dd>
<p>Set Top_p value, nucleus sampling (cmpls/chat, 0.0 - 1.0).</p>
</dd>
<dt><strong>-r</strong>, <strong>--restart</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Set restart sequence string (cmpls).</p>
</dd>
<dt><strong>-R</strong>, <strong>--start</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Set start sequence string (cmpls).</p>
</dd>
<dt><strong>-s</strong>, <strong>--stop</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Set stop sequences, up to 4. Def="<em>&lt;|endoftext|&gt;</em>".</p>
</dd>
<dt><strong>-S</strong>, <strong>--instruction</strong>
[<em>INSTRUCTION</em>|<em>FILE</em>]</dt>
<dd>
<p>Set an instruction text prompt. It may be a text file.</p>
</dd>
<dt><strong>-t</strong>, <strong>--temperature</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Set temperature value (cmpls/chat/whisper), (0.0 - 2.0, whisper 0.0 -
1.0). Def=<em>0</em>.</p>
</dd>
</dl>
<h4 id="miscellaneous-settings">Miscellaneous Settings</h4>
<dl>
<dt><strong>--api-key</strong> [<em>KEY</em>]</dt>
<dd>
<p>Set API key to use.</p>
</dd>
<dt><strong>--fold</strong> (<em>defaults</em>),
<strong>--no-fold</strong></dt>
<dd>
<p>Set or unset response folding (wrap at white spaces).</p>
</dd>
<dt><strong>-h</strong>, <strong>--help</strong></dt>
<dd>
<p>Print the help page.</p>
</dd>
<dt><strong>-k</strong>, <strong>--no-colour</strong></dt>
<dd>
<p>Disable colour output. Def=<em>auto</em>.</p>
</dd>
<dt><strong>-l</strong>, <strong>--list-models</strong>
[<em>MODEL</em>]</dt>
<dd>
<p>List models or print details of <em>MODEL</em>.</p>
</dd>
<dt><strong>-L</strong>, <strong>--log</strong> [<em>FILEPATH</em>]</dt>
<dd>
<p>Set log file. <em>FILEPATH</em> is required.</p>
</dd>
<dt><strong>--md</strong>, <strong>--markdown</strong>,
<strong>--markdown</strong>=[<em>SOFTWARE</em>]</dt>
<dd>
<p>Enable markdown rendering in response. Software is optional:
<em>bat</em>, <em>pygmentize</em>, <em>glow</em>, <em>mdcat</em>, or
<em>mdless</em>.</p>
</dd>
<dt><strong>--no-md</strong>, <strong>--no-markdown</strong></dt>
<dd>
<p>Disable markdown rendering.</p>
</dd>
<dt><strong>-o</strong>, <strong>--clipboard</strong></dt>
<dd>
<p>Copy response to clipboard.</p>
</dd>
<dt><strong>-v</strong>, <strong>--verbose</strong></dt>
<dd>
<p>Less verbose.</p>
<p>Sleep after response in voice chat (<code>-vvccw</code>).</p>
<p>With <code>options -ccwv</code>, sleep after response. With
<code>options -ccwzvv</code>, stop recording voice input on silence
detection and play TTS response right away.</p>
<p>May be set multiple times.</p>
</dd>
<dt><strong>-V</strong></dt>
<dd>
<p>Dump raw JSON request block (debug).</p>
</dd>
<dt><strong>--version</strong></dt>
<dd>
<p>Print script version.</p>
</dd>
<dt><strong>-y</strong>, <strong>--tik</strong></dt>
<dd>
<p>Set tiktoken for token count (cmpls/chat, python).</p>
</dd>
<dt><strong>-Y</strong>, <strong>--no-tik</strong>
(<em>defaults</em>)</dt>
<dd>
<p>Unset tiktoken use (cmpls/chat, python).</p>
</dd>
<dt><strong>-Z</strong>, <strong>-ZZ</strong>, <strong>-ZZZ</strong>,
<strong>--last</strong></dt>
<dd>
<p>Print JSON data of the last responses.</p>
</dd>
</dl>
</body>
</html>
