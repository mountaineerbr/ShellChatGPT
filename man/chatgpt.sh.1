'\" t
.\" Automatically generated by Pandoc 3.1.8
.\"
.TH "CHATGPT.SH" "1" "July 2024" "v0.67.9" "General Commands Manual"
.SS NAME
.PP
\ \ \ chatgpt.sh -- Wrapper for ChatGPT / DALL-E / Whisper / TTS
.SS SYNOPSIS
.PP
\ \ \ \f[B]chatgpt.sh\f[R] [\f[CR]-cc\f[R]|\f[CR]-d\f[R]|\f[CR]-qq\f[R]]
[\f[CR]opt\f[R]..]
[\f[I]PROMPT\f[R]|\f[I]TEXT_FILE\f[R]|\f[I]PDF_FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-i\f[R] [\f[CR]opt\f[R]..]
[\f[I]X\f[R]|\f[I]L\f[R]|\f[I]P\f[R]][\f[I]hd\f[R]] [\f[I]PROMPT\f[R]]
#\f[I]Dall-E-3\f[R]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-i\f[R] [\f[CR]opt\f[R]..]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-i\f[R] [\f[CR]opt\f[R]..]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PNG_FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-i\f[R] [\f[CR]opt\f[R]..]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PNG_FILE\f[R]]
[\f[I]MASK_FILE\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-w\f[R] [\f[CR]opt\f[R]..]
[\f[I]AUDIO_FILE\f[R]|\f[I].\f[R]] [\f[I]LANG\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-W\f[R] [\f[CR]opt\f[R]..]
[\f[I]AUDIO_FILE\f[R]|\f[I].\f[R]] [\f[I]PROMPT-EN\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-z\f[R] [\f[CR]opt\f[R]..]
[\f[I]OUTFILE\f[R]|\f[I]FORMAT\f[R]|\f[I]-\f[R]] [\f[I]VOICE\f[R]]
[\f[I]SPEED\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-ccWwz\f[R] [\f[CR]opt\f[R]..]
-- [\f[I]PROMPT\f[R]] -- [\f[CR]whisper_arg\f[R]..]
-- [\f[CR]tts_arg\f[R]..]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-l\f[R] [\f[I]MODEL\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-TTT\f[R] [-v]
[\f[CR]-m\f[R][\f[I]MODEL\f[R]|\f[I]ENCODING\f[R]]]
[\f[I]INPUT\f[R]|\f[I]TEXT_FILE\f[R]|\f[I]PDF_FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-HHPP\f[R]
[\f[CR]/\f[R]\f[I]HIST_FILE\f[R]|\f[I].\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[CR]-HHPw\f[R]
.SS DESCRIPTION
With no options set, complete INPUT in single-turn mode of plain text
completions.
.PP
\f[CR]Option -d\f[R] starts a multi-turn session in \f[B]plain text
completions\f[R].
This does not set further options automatically.
.PP
Set \f[CR]option -c\f[R] to start a multi-turn chat mode via \f[B]text
completions\f[R] and record conversation.
This option accepts davinci and lesser models, defaults to
\f[I]gpt-3.5-turbo-instruct\f[R] if none set.
In chat mode, some options are automatically set to un-lobotomise the
bot.
Set \f[CR]option -E\f[R] to exit on the first response.
.PP
Set \f[CR]option -cc\f[R] to start the chat mode via \f[B]native chat
completions\f[R] and defaults to \f[I]gpt-4o\f[R].
.PP
Set \f[CR]option -C\f[R] to \f[B]resume\f[R] (continue from) last
history session.
.PP
Set \f[CR]option -q\f[R] for \f[B]insert mode\f[R].
The flag \[lq]\f[I][insert]\f[R]\[rq] must be present in the middle of
the input prompt.
Insert mode works completing between the end of the text preceding the
flag, and ends completion with the succeeding text after the flag.
Insert mode works with model \f[CR]gpt-3.5-turbo-instruct\f[R].
.PP
Positional arguments are read as a single \f[B]PROMPT\f[R].
Model \f[B]INSTRUCTION\f[R] is usually optional and can be set with
\f[CR]option -S\f[R].
.PP
In multi-turn interactions, prompts prefixed with a single colon
\[lq]\f[I]:\f[R]\[rq] are appended to the current request buffer as user
messages without making a new API call.
Conversely, prompts starting with double colons \[lq]\f[I]::\f[R]\[rq]
are appended as instruction / system messages.
For text cmpls only, triple colons append the text immediately to the
previous prompt without a restart sequence.
.PP
If a plain text or PDF file path is set as the first positional
argument, or as an argument to \f[CR]option -S\f[R] (set instruction
prompt), the file is loaded as text PROMPT.
.PP
With \f[B]vision models\f[R], insert an image to the prompt with chat
command \[lq]\f[CR]!img\f[R] [\f[I]url\f[R]|\f[I]filepath\f[R]]\[rq].
Image urls and files can also be appended by typing the operator pipe
and a valid input at the end of the text prompt, such as
\[lq]\f[CR]|\f[R] [\f[I]url\f[R]|\f[I]filepath\f[R]]\[rq].
.PP
To create and reuse a custom prompt, set the prompt name as a command
line option, such as \[lq]\f[CR]-S .[_prompt_name_]\f[R]\[rq] or
\[lq]\f[CR]-S ..[_prompt_name_]\f[R]\[rq].
Note that loading a custom prompt will also change to its respective
history file.
.PP
Alternatively, set the first positional argument with the operator plus
the name, such as \[lq]\f[CR]..[_prompt_]\f[R]\[rq], unless instruction
was set manually.
.PP
If the first positional argument of the script starts with the command
operator forward slash \[lq]\f[CR]/\f[R]\[rq] and a history file name,
the command \[lq]\f[CR]/session\f[R] [\f[I]HIST_NAME\f[R]]\[rq] is
assumed.
This will change to or create a new history file (with
\f[CR]options -ccCdHH\f[R]).
.PP
Set model with \[lq]\f[CR]-m\f[R] [\f[I]MODEL\f[R]]\[rq], with
\f[I]MODEL\f[R] as its name, or set it as \[lq]\f[I].\f[R]\[rq] to pick
from the model list.
List available models with \f[CR]option -l\f[R].
.PP
Set \f[I]maximum response tokens\f[R] with \f[CR]option\f[R]
\[lq]\f[CR]-\f[R]\f[I]NUM\f[R]\[rq] or \[lq]\f[CR]-M\f[R]
\f[I]NUM\f[R]\[rq].
This defaults to \f[I]1024\f[R] tokens.
.PP
If a second \f[I]NUM\f[R] is given to this option, \f[I]maximum model
capacity\f[R] will also be set.
The option syntax takes the form of
\[lq]\f[CR]-\f[R]\f[I]NUM/NUM\f[R]\[rq], and \[lq]\f[CR]-M\f[R]
\f[I]NUM-NUM\f[R]\[rq].
.PP
\f[I]Model capacity\f[R] (maximum model tokens) can be set more
intuitively with \f[CR]option\f[R] \[lq]\f[CR]-N\f[R]
\f[I]NUM\f[R]\[rq], otherwise model capacity is set automatically for
known models, or to \f[I]2048\f[R] tokens as fallback.
.PP
\f[CR]Option -S\f[R] sets an INSTRUCTION prompt (the initial prompt) for
text cmpls, and chat cmpls.
A text file path may be supplied as the single argument.
Also see \f[I]CUSTOM / AWESOME PROMPTS\f[R] section below.
.PP
\f[CR]Option -i\f[R] \f[B]generates images\f[R] according to text
PROMPT.
If the first positional argument is an \f[I]IMAGE\f[R] file, then
\f[B]generate variations\f[R] of it.
If the first positional argument is an \f[I]IMAGE\f[R] file and the
second a \f[I]MASK\f[R] file (with alpha channel and transparency), and
a text PROMPT (required), then \f[B]edit the\f[R] \f[I]IMAGE\f[R]
according to \f[I]MASK\f[R] and PROMPT.
If \f[I]MASK\f[R] is not provided, \f[I]IMAGE\f[R] must have
transparency.
.PP
The \f[B]size of output images\f[R] may be set as the first positional
parameter in the command line: \[lq]\f[I]256x256\f[R]\[rq]
(\f[I]S\f[R]), \[lq]\f[I]512x512\f[R]\[rq] (\f[I]M\f[R]),
\[lq]\f[I]1024x1024\f[R]\[rq] (\f[I]L\f[R]),
\[lq]\f[I]1792x1024\f[R]\[rq] (\f[I]X\f[R]), and
\[lq]\f[I]1024x1792\f[R]\[rq] (\f[I]P\f[R]).
.PP
The parameter \[lq]\f[I]hd\f[R]\[rq] may also be set for image quality
(\f[I]Dall-E-3\f[R]), such as \[lq]\f[I]Xhd\f[R]\[rq], or
\[lq]\f[I]1792x1024hd\f[R]\[rq].
Defaults=\f[I]1024x1024\f[R].
.PP
See \f[B]IMAGES section\f[R] below for more information on
\f[B]inpaint\f[R] and \f[B]outpaint\f[R].
.PP
\f[CR]Option -w\f[R] \f[B]transcribes audio\f[R] from \f[I]mp3\f[R],
\f[I]mp4\f[R], \f[I]mpeg\f[R], \f[I]mpga\f[R], \f[I]m4a\f[R],
\f[I]wav\f[R], and \f[I]webm\f[R] files.
First positional argument must be an \f[I]AUDIO\f[R] file.
Optionally, set a \f[I]TWO-LETTER\f[R] input language
(\f[I]ISO-639-1\f[R]) as the second argument.
A PROMPT may also be set to guide the model\[cq]s style, or continue a
previous audio segment.
The text prompt should match the audio language.
.PP
Note that \f[CR]option -w\f[R] can also be set to \f[B]translate
audio\f[R] input to any text language to the target language.
.PP
\f[CR]Option -W\f[R] \f[B]translates audio\f[R] stream to \f[B]English
text\f[R].
A PROMPT in English may be set to guide the model as the second
positional argument.
.PP
Set these options twice, e.g.\ \f[CR]-ww\f[R], and \f[CR]-WW\f[R], to
have phrase-level timestamps.
.PP
Combine \f[CR]options -wW\f[R] \f[B]with\f[R] \f[CR]options -cc\f[R] to
start \f[B]chat with voice input\f[R] (Whisper) support.
Additionally, set \f[CR]option -z\f[R] to enable
\f[B]text-to-speech\f[R] (TTS) models and voice out.
.PP
\f[CR]Option -z\f[R] synthesises voice from text (TTS models).
Set a \f[I]voice\f[R] as the first positional parameter
(\[lq]\f[I]alloy\f[R]\[rq], \[lq]\f[I]echo\f[R]\[rq],
\[lq]\f[I]fable\f[R]\[rq], \[lq]\f[I]onyx\f[R]\[rq],
\[lq]\f[I]nova\f[R]\[rq], or \[lq]\f[I]shimmer\f[R]\[rq]).
Set the second positional parameter as the \f[I]voice speed\f[R]
(\f[I]0.25\f[R] - \f[I]4.0\f[R]), and, finally the \f[I]output file
name\f[R] or the \f[I]format\f[R], such as
\[lq]\f[I]./new_audio.mp3\f[R]\[rq] (\[lq]\f[I]mp3\f[R]\[rq],
\[lq]\f[I]opus\f[R]\[rq], \[lq]\f[I]aac\f[R]\[rq], and
\[lq]\f[I]flac\f[R]\[rq]), or \[lq]\f[I]-\f[R]\[rq] for stdout.
Set \f[CR]options -vz\f[R] to \f[I]not\f[R] play received output.
.PP
\f[CR]Option -y\f[R] sets python tiktoken instead of the default script
hack to preview token count.
This option makes token count preview accurate fast (we fork tiktoken as
a coprocess for fast token queries).
Useful for rebuilding history context independently from the original
model used to generate responses.
.PP
The moderation endpoint can be accessed by setting the model name to
\f[I]text-moderation-latest\f[R].
.PP
Stdin text is appended to PROMPT, to set a single PROMPT.
.PP
While \f[I]cURL\f[R] is in the middle of transmitting a request, or
receiving a response, <\f[I]CTRL-C\f[R]> may be pressed once to
interrupt the call.
.PP
Press <\f[I]CTRL-X\f[R] \f[I]CTRL-E\f[R]> to edit command line in text
editor (readline).
.PP
Press <\f[I]CTRL-J\f[R]> or <\f[I]CTRL-V\f[R] \f[I]CTRL-J\f[R]> for
newline (readline).
.PP
Press <\f[I]CTRL-\[rs]\f[R]> to exit from the script, even if recording,
requesting, or playing TTS.
.PP
User configuration is kept at \[lq]\f[I]\[ti]/.chatgpt.conf\f[R]\[rq].
Script cache is kept at \[lq]\f[I]\[ti]/.cache/chatgptsh/\f[R]\[rq].
.PP
A personal OpenAI API is required, set it with
\f[CR]option --api-key\f[R].
See also \f[B]ENVIRONMENT section\f[R].
.PP
This script also supports warping LocalAI, Ollama, Gemini and Mistral
APIs.
.PP
For LocalAI integration, run the script with
\f[CR]option --localai\f[R], or set environment
\f[B]$OPENAI_API_HOST\f[R] with the server URL.
.PP
For Mistral AI set environment variable \f[B]$MISTRAL_API_KEY\f[R], and
run the script with \f[CR]option --mistral\f[R] or set
\f[B]$OPENAI_API_HOST\f[R] to \[lq]https://api.mistral.ai/\[rq].
Prefer setting command line \f[CR]option --mistral\f[R] for complete
integration.
.PP
List models with \f[CR]option -l\f[R], or run \f[CR]/models\f[R] in chat
mode.
.PP
For Ollama, set \f[CR]option -O\f[R] (\f[CR]--ollama\f[R]), and set
\f[B]$OLLAMA_API_HOST\f[R] if the server URL is different from the
defaults.
.PP
Note that model management (downloading and setting up) must follow the
Ollama project guidelines and own methods.
.PP
For Google Gemini, set environment variable \f[B]$GOOGLE_API_KEY\f[R],
and run the script with the command line \f[CR]option --google\f[R].
.PP
Command \[lq]\f[CR]!block\f[R] [\f[I]args\f[R]]\[rq] may be run to set
raw model options in JSON syntax according to each API.
Alternatively, set envar \f[B]$BLOCK_USR\f[R].
.PP
For complete model and settings information, refer to OpenAI API docs at
<https://platform.openai.com/docs/>.
.PP
See the online man page and \f[CR]chatgpt.sh\f[R] usage examples at:
<https://gitlab.com/fenixdragao/shellchatgpt>.
.SS TEXT / CHAT COMPLETIONS
.SS 1. Text completions
Given a prompt, the model will return one or more predicted completions.
For example, given a partial input, the language model will try
completing it until probable \[lq]\f[CR]<|endoftext|>\f[R]\[rq], or
other stop sequences (stops may be set with \f[CR]-s\f[R]).
.PP
\f[B]Restart\f[R] and \f[B]start sequences\f[R] may be optionally set
and are always preceded by a new line.
.PP
To enable \f[B]multiline input\f[R], set \f[CR]option -u\f[R].
With this option set, press <\f[I]CTRL-D\f[R]> to flush input!
This is useful to paste from clipboard.
Alternatively, set \f[CR]option -U\f[R] to set \f[I]cat command\f[R] as
prompter.
.PP
Bash bracketed paste is enabled, meaning multiline input may be pasted
or typed, even without setting \f[CR]options -uU\f[R]
(\f[I]v25.2+\f[R]).
.PP
Language model \f[B]SKILLS\f[R] can activated, with specific prompts,
see <https://platform.openai.com/examples>.
.SS 2. Chat Mode
.SS 2.1 Text Completions Chat
Set \f[CR]option -c\f[R] to start chat mode of text completions.
It keeps a history file, and keeps new questions in context.
This works with a variety of models.
Set \f[CR]option -E\f[R] to exit on response.
.SS 2.2 Native Chat Completions
Set the double \f[CR]option -cc\f[R] to start chat completions mode.
Turbo models are also the best option for many non-chat use cases.
.SS 2.3 Q & A Format
The defaults chat format is \[lq]\f[B]Q & A\f[R]\[rq].
The \f[B]restart sequence\f[R] \[lq]\f[I]\[rs]nQ:\ \f[R]\[rq] and the
\f[B]start text\f[R] \[lq]\f[I]\[rs]nA:\f[R]\[rq] are injected for the
chat bot to work well with text cmpls.
.PP
In native chat completions, setting a prompt with \[lq]\f[I]:\f[R]\[rq]
as the initial character sets the prompt as a \f[B]SYSTEM\f[R] message.
In text completions, however, typing a colon \[lq]\f[I]:\f[R]\[rq] at
the start of the prompt causes the text following it to be appended
immediately to the last (response) prompt text.
.SS 2.4 Voice input (Whisper), and voice output (TTS)
The \f[CR]options -ccwz\f[R] may be combined to have voice recording
input and synthesised voice output, specially nice with chat modes.
When setting \f[CR]flag -w\f[R], or \f[CR]flag -z\f[R], the first
positional parameters are read as Whisper, or TTS arguments.
When setting both \f[CR]flags -wz\f[R], add a double hyphen to set first
Whisper, and then TTS arguments.
.PP
Set chat mode, plus Whisper language and prompt, and the TTS voice
option argument:
.IP
.EX
chatgpt.sh -ccwz  en \[aq]whisper prompt\[aq]  --  nova
.EE
.SS 2.5 GPT-4-Vision
To send an \f[I]image\f[R], or \f[I]url\f[R] to \f[B]vision models\f[R],
either set the image with the \f[CR]!img\f[R] chat command with one or
more \f[I]filepaths\f[R] / \f[I]urls\f[R] separated by the operator pipe
\f[I]|\f[R].
.IP
.EX
chatgpt.sh -cc -m gpt-4-vision-preview \[aq]!img path/to/image.jpg\[aq]
.EE
.PP
Alternatively, set the \f[I]image paths\f[R] / \f[I]urls\f[R] at the end
of the text prompt interactively:
.IP
.EX
chatgpt.sh -cc -m gpt-4-vision-preview

[...]
Q: In this first user prompt, what can you see? | https://i.imgur.com/wpXKyRo.jpeg
.EE
.SS 2.6 Chat Commands
While in chat mode, the following commands can be typed in the new
prompt to set a new parameter.
The command operator may be either \[lq]\f[CR]!\f[R]\[rq], or
\[lq]\f[CR]/\f[R]\[rq].
.PP
.TS
tab(@);
l l l.
T{
Misc
T}@T{
Commands
T}@T{
T}
_
T{
\f[CR]-S\f[R]
T}@T{
\f[CR]:\f[R], \f[CR]::\f[R] [\f[I]PROMPT\f[R]]
T}@T{
Append user or system prompt to request buffer.
T}
T{
\f[CR]-S.\f[R]
T}@T{
\f[CR]-.\f[R] [\f[I]NAME\f[R]]
T}@T{
Load and edit custom prompt.
T}
T{
\f[CR]-S/\f[R]
T}@T{
\f[CR]-S%\f[R] [\f[I]NAME\f[R]]
T}@T{
Load and edit awesome prompt (zh).
T}
T{
\f[CR]-Z\f[R]
T}@T{
\f[CR]!last\f[R]
T}@T{
Print last response JSON.
T}
T{
\f[CR]!\f[R]
T}@T{
\f[CR]!r\f[R], \f[CR]!regen\f[R]
T}@T{
Regenerate last response.
T}
T{
\f[CR]!!\f[R]
T}@T{
\f[CR]!rr\f[R]
T}@T{
Regenerate response, edit prompt first.
T}
T{
\f[CR]!i\f[R]
T}@T{
\f[CR]!info\f[R]
T}@T{
Information on model and session settings.
T}
T{
\f[CR]!j\f[R]
T}@T{
\f[CR]!jump\f[R]
T}@T{
Jump to request, append start seq primer (text cmpls).
T}
T{
\f[CR]!!j\f[R]
T}@T{
\f[CR]!!jump\f[R]
T}@T{
Jump to request, no response priming.
T}
T{
\f[CR]!md\f[R]
T}@T{
\f[CR]!markdown\f[R] [\f[I]SOFTW\f[R]]
T}@T{
Toggle markdown rendering in response.
T}
T{
\f[CR]!!md\f[R]
T}@T{
\f[CR]!!markdown\f[R] [\f[I]SOFTW\f[R]]
T}@T{
Render last response in markdown.
T}
T{
\f[CR]!rep\f[R]
T}@T{
\f[CR]!replay\f[R]
T}@T{
Replay last TTS audio response.
T}
T{
\f[CR]!res\f[R]
T}@T{
\f[CR]!resubmit\f[R]
T}@T{
Resubmit last TTS recorded input from cache.
T}
T{
\f[CR]!cat\f[R]
T}@T{
-
T}@T{
Cat prompter as one-shot, <\f[I]CTRL-D\f[R]> flush.
T}
T{
\f[CR]!cat\f[R]
T}@T{
\f[CR]!cat:\f[R] [\f[I]TXT\f[R]|\f[I]URL\f[R]|\f[I]PDF\f[R]]
T}@T{
Cat \f[I]text\f[R] or \f[I]PDF\f[R] file, or dump \f[I]URL\f[R]
(\f[I]:\f[R] to append).
T}
T{
!dialog\[ga]
T}@T{
-
T}@T{
Toggle the \[lq]dialog\[rq] interface.
T}
T{
\f[CR]!img\f[R]
T}@T{
\f[CR]!media\f[R] [\f[I]FILE\f[R]|\f[I]URL\f[R]]
T}@T{
Append image, media, or URL to prompt.
T}
T{
\f[CR]!p\f[R]
T}@T{
\f[CR]!pick\f[R], [\f[I]PROPMT\f[R]]\f[CR]!p\f[R]
T}@T{
File picker (at start or end of prompt).
T}
T{
\f[CR]!pdf\f[R]
T}@T{
\f[CR]!pdf:\f[R] [\f[I]FILE\f[R]]
T}@T{
Convert PDF and dump text (\f[I]:\f[R] to append prompt).
T}
T{
\f[CR]!photo\f[R]
T}@T{
\f[CR]!!photo\f[R] [\f[I]INDEX\f[R]]
T}@T{
Take a photo, optionally set camera index (Termux only).
T}
T{
\f[CR]!sh\f[R]
T}@T{
\f[CR]!shell\f[R] [\f[I]CMD\f[R]]
T}@T{
Run shell, or \f[I]command\f[R], and edit output.
T}
T{
\f[CR]!sh:\f[R]
T}@T{
\f[CR]!shell:\f[R] [\f[I]CMD\f[R]]
T}@T{
Same as \f[CR]!sh\f[R] but apppend output as user.
T}
T{
\f[CR]!!sh\f[R]
T}@T{
\f[CR]!!shell\f[R] [\f[I]CMD\f[R]]
T}@T{
Run interactive shell (with \f[I]command\f[R]) and exit.
T}
T{
\f[CR]!url\f[R]
T}@T{
\f[CR]!url:\f[R] [\f[I]URL\f[R]]
T}@T{
Dump URL text (add \f[I]:\f[R] to append prompt).
T}
.TE
.PP
.TS
tab(@);
l l l.
T{
Script
T}@T{
Settings and UX
T}@T{
T}
_
T{
\f[CR]!fold\f[R]
T}@T{
\f[CR]!wrap\f[R]
T}@T{
Toggle response wrapping.
T}
T{
\f[CR]-g\f[R]
T}@T{
\f[CR]!stream\f[R]
T}@T{
Toggle response streaming.
T}
T{
\f[CR]-h\f[R]
T}@T{
\f[CR]!help\f[R] [\f[I]REGEX\f[R]]
T}@T{
Print help snippet or grep help for regex.
T}
T{
\f[CR]-l\f[R]
T}@T{
\f[CR]!models\f[R] [\f[I]NAME\f[R]]
T}@T{
List language models or show model details.
T}
T{
\f[CR]-o\f[R]
T}@T{
\f[CR]!clip\f[R]
T}@T{
Copy responses to clipboard.
T}
T{
\f[CR]-u\f[R]
T}@T{
\f[CR]!multi\f[R]
T}@T{
Toggle multiline prompter.
<\f[I]CTRL-D\f[R]> flush.
T}
T{
\f[CR]-uu\f[R]
T}@T{
\f[CR]!!multi\f[R]
T}@T{
Multiline, one-shot.
<\f[I]CTRL-D\f[R]> flush.
T}
T{
\f[CR]-U\f[R]
T}@T{
\f[CR]-UU\f[R]
T}@T{
Toggle cat prompter, or set one-shot.
<\f[I]CTRL-D\f[R]> flush.
T}
T{
\f[CR]-V\f[R]
T}@T{
\f[CR]!context\f[R]
T}@T{
Print context before request (see \f[CR]option -P\f[R]).
T}
T{
\f[CR]-VV\f[R]
T}@T{
\f[CR]!debug\f[R]
T}@T{
Dump raw request block and confirm.
T}
T{
\f[CR]-v\f[R]
T}@T{
\f[CR]!ver\f[R]
T}@T{
Toggle verbose modes.
T}
T{
\f[CR]-x\f[R]
T}@T{
\f[CR]!ed\f[R]
T}@T{
Toggle text editor interface.
T}
T{
\f[CR]-xx\f[R]
T}@T{
\f[CR]!!ed\f[R]
T}@T{
Single-shot text editor.
T}
T{
\f[CR]-y\f[R]
T}@T{
\f[CR]!tik\f[R]
T}@T{
Toggle python tiktoken use.
T}
T{
\f[CR]!q\f[R]
T}@T{
\f[CR]!quit\f[R]
T}@T{
Exit.
Bye.
T}
.TE
.PP
.TS
tab(@);
l l l.
T{
Model
T}@T{
Settings
T}@T{
T}
_
T{
\f[CR]-Nill\f[R]
T}@T{
\f[CR]!Nill\f[R]
T}@T{
Toggle model max response (chat cmpls).
T}
T{
\f[CR]-M\f[R]
T}@T{
\f[CR]!NUM\f[R] \f[CR]!max\f[R] [\f[I]NUM\f[R]]
T}@T{
Set maximum response tokens.
T}
T{
\f[CR]-N\f[R]
T}@T{
\f[CR]!modmax\f[R] [\f[I]NUM\f[R]]
T}@T{
Set model token capacity.
T}
T{
\f[CR]-a\f[R]
T}@T{
\f[CR]!pre\f[R] [\f[I]VAL\f[R]]
T}@T{
Set presence penalty.
T}
T{
\f[CR]-A\f[R]
T}@T{
\f[CR]!freq\f[R] [\f[I]VAL\f[R]]
T}@T{
Set frequency penalty.
T}
T{
\f[CR]-b\f[R]
T}@T{
\f[CR]!best\f[R] [\f[I]NUM\f[R]]
T}@T{
Set best-of n results.
T}
T{
\f[CR]-K\f[R]
T}@T{
\f[CR]!topk\f[R] [\f[I]NUM\f[R]]
T}@T{
Set top_k.
T}
T{
\f[CR]-m\f[R]
T}@T{
\f[CR]!mod\f[R] [\f[I]MOD\f[R]]
T}@T{
Set model by name, empty to pick from list.
T}
T{
\f[CR]-n\f[R]
T}@T{
\f[CR]!results\f[R] [\f[I]NUM\f[R]]
T}@T{
Set number of results.
T}
T{
\f[CR]-p\f[R]
T}@T{
\f[CR]!topp\f[R] [\f[I]VAL\f[R]]
T}@T{
Set top_p.
T}
T{
\f[CR]-r\f[R]
T}@T{
\f[CR]!restart\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set restart sequence.
T}
T{
\f[CR]-R\f[R]
T}@T{
\f[CR]!start\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set start sequence.
T}
T{
\f[CR]-s\f[R]
T}@T{
\f[CR]!stop\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set one stop sequence.
T}
T{
\f[CR]-t\f[R]
T}@T{
\f[CR]!temp\f[R] [\f[I]VAL\f[R]]
T}@T{
Set temperature.
T}
T{
\f[CR]-w\f[R]
T}@T{
\f[CR]!rec\f[R] [\f[I]ARGS\f[R]]
T}@T{
Toggle Whisper.
Optionally, set arguments.
T}
T{
\f[CR]-z\f[R]
T}@T{
\f[CR]!tts\f[R] [\f[I]ARGS\f[R]]
T}@T{
Toggle TTS chat mode (speech out).
T}
T{
\f[CR]!ka\f[R]
T}@T{
\f[CR]!keep-alive\f[R] [\f[I]NUM\f[R]]
T}@T{
Set duration of model load in memory (Ollama).
T}
T{
\f[CR]!blk\f[R]
T}@T{
\f[CR]!block\f[R] [\f[I]ARGS\f[R]]
T}@T{
Set and add custom options to JSON request.
T}
T{
-
T}@T{
\f[CR]!multimodal\f[R]
T}@T{
Toggle model as multimodal.
T}
.TE
.PP
.TS
tab(@);
l l l.
T{
Session
T}@T{
Management
T}@T{
T}
_
T{
\f[CR]-H\f[R]
T}@T{
\f[CR]!hist\f[R]
T}@T{
Edit history in editor.
T}
T{
\f[CR]-P\f[R]
T}@T{
\f[CR]-HH\f[R], \f[CR]!print\f[R]
T}@T{
Print session history (see \f[CR]option -V\f[R]).
T}
T{
\f[CR]-L\f[R]
T}@T{
\f[CR]!log\f[R] [\f[I]FILEPATH\f[R]]
T}@T{
Save to log file.
T}
T{
\f[CR]!br\f[R]
T}@T{
\f[CR]!break\f[R], \f[CR]!new\f[R]
T}@T{
Start new session (session break).
T}
T{
\f[CR]!ls\f[R]
T}@T{
\f[CR]!list\f[R] [\f[I]GLOB\f[R]]
T}@T{
List History files with \f[I]name\f[R] \f[I]glob\f[R],
T}
T{
T}@T{
T}@T{
Prompts \[lq]\f[I]pr\f[R]\[rq], Awesome \[lq]\f[I]awe\f[R]\[rq], or all
files \[lq]\f[I].\f[R]\[rq].
T}
T{
\f[CR]!grep\f[R]
T}@T{
\f[CR]!sub\f[R] [\f[I]REGEX\f[R]]
T}@T{
Search sessions (for regex) and copy session to hist tail.
T}
T{
\f[CR]!c\f[R]
T}@T{
\f[CR]!copy\f[R] [\f[I]SRC_HIST\f[R]] [\f[I]DEST_HIST\f[R]]
T}@T{
Copy session from source to destination.
T}
T{
\f[CR]!f\f[R]
T}@T{
\f[CR]!fork\f[R] [\f[I]DEST_HIST\f[R]]
T}@T{
Fork current session to destination.
T}
T{
\f[CR]!k\f[R]
T}@T{
\f[CR]!kill\f[R] [\f[I]NUM\f[R]]
T}@T{
Comment out \f[I]n\f[R] last entries in history file.
T}
T{
\f[CR]!!k\f[R]
T}@T{
\f[CR]!!kill\f[R] [[\f[I]0\f[R]]\f[I]NUM\f[R]]
T}@T{
Dry-run of command \f[CR]!kill\f[R].
T}
T{
\f[CR]!s\f[R]
T}@T{
\f[CR]!session\f[R] [\f[I]HIST_FILE\f[R]]
T}@T{
Change to, search for, or create history file.
T}
T{
\f[CR]!!s\f[R]
T}@T{
\f[CR]!!session\f[R] [\f[I]HIST_FILE\f[R]]
T}@T{
Same as \f[CR]!session\f[R], break session.
T}
.TE
.PP
E.g.: \[lq]\f[CR]/temp\f[R] \f[I]0.7\f[R]\[rq],
\[lq]\f[CR]!mod\f[R]\f[I]gpt-4\f[R]\[rq], \[lq]\f[CR]-p\f[R]
\f[I]0.2\f[R]\[rq], and \[lq]\f[CR]/s\f[R] \f[I]hist_name\f[R]\[rq].
.SS 2.7 Session Management
The script uses a \f[I]TSV file\f[R] to record entries, which is kept at
the script cache directory.
A new history file can be created, or an existing one changed to with
command \[lq]\f[CR]/session\f[R] [\f[I]HIST_FILE\f[R]]\[rq], in which
\f[I]HIST_FILE\f[R] is the file name of (with or without the
\f[I].tsv\f[R] extension), or path to, a history file.
.PP
When the first positional argument to the script is the command operator
forward slash followed by a history file name, the command
\f[CR]/session\f[R] is assumed.
.PP
A history file can contain many sessions.
The last one (the tail session) is always loaded if the resume
\f[CR]option -C\f[R] is set.
.PP
To copy a previous session, run \f[CR]/sub\f[R], or
\f[CR]/grep [regex]\f[R] to copy that session to tail and resume from
it.
.PP
If \[lq]\f[CR]/copy\f[R] \f[I]current\f[R]\[rq] is run, a selector is
shown to choose and copy a session to the tail of the current history
file, and resume it.
This is equivalent to running \[lq]\f[CR]/fork\f[R]\[rq].
.PP
It is also possible to copy sessions of a history file to another file
when a second argument is given to the command with the history file
name, such as \[lq]\f[CR]/copy\f[R] [\f[I]SRC_HIST_FILE\f[R]]
[\f[I]DEST_HIST_FILE\f[R]]\[rq].
.PP
To load an older session from a history file that is different from the
defaults, there are some options.
.PP
Change to it with command \f[CR]!session [name]\f[R], and then
\f[CR]!fork\f[R] the older session to the active session.
.PP
Or, \f[CR]!copy [orign] [dest]\f[R] the session from a history file to
the current oneor any other history file.
.PP
In these cases, a pickup interface should open to let the user choose
the correct session from the history file.
.PP
To change the chat context at run time, the history file may be edited
with the \[lq]\f[CR]/hist\f[R]\[rq] command (also for context
injection).
Delete history entries or comment them out with \[lq]\f[CR]#\f[R]\[rq].
.SS 2.8 Completion Preview / Regeneration
To preview a prompt completion before committing it to history, append a
forward slash \[lq]\f[CR]/\f[R]\[rq] to the prompt as the last
character.
Regenerate it again or flush/accept the prompt and response.
.PP
After a response has been written to the history file,
\f[B]regenerate\f[R] it with command \[lq]\f[CR]!regen\f[R]\[rq] or type
in a single exclamation mark or forward slash \[lq]\f[CR]/\f[R]\[rq] in
the new empty prompt (twice \[lq]\f[CR]//\f[R]\[rq] for editing the
prompt before new request).
.SS 3. Prompt Engineering and Design
Minimal \f[B]INSTRUCTION\f[R] to behave like a chatbot is given with
chat \f[CR]options -cc\f[R], unless otherwise explicitly set by the
user.
.PP
On chat mode, if no INSTRUCTION is set, minimal instruction is given,
and some options auto set, such as increasing temp and presence penalty,
in order to un-lobotomise the bot.
With cheap and fast models of text cmpls, such as Curie, the
\f[CR]best_of\f[R] option may be worth setting (to 2 or 3).
.PP
Prompt engineering is an art on itself.
Study carefully how to craft the best prompts to get the most out of
text, code and chat cmpls models.
.PP
Certain prompts may return empty responses.
Maybe the model has nothing to further complete input or it expects more
text.
Try trimming spaces, appending a full stop/ellipsis, resetting
temperature, or adding more text.
.PP
Prompts ending with a space character may result in lower quality
output.
This is because the API already incorporates trailing spaces in its
dictionary of tokens.
.PP
Note that the model\[cq]s steering and capabilities require prompt
engineering to even know that it should answer the questions.
.PP
It is also worth trying to sample 3 - 5 times (increasing the number of
responses with option \f[CR]-n 3\f[R], for example) in order to obtain a
good response.
.PP
For more on prompt design, see:
.IP \[bu] 2
<https://platform.openai.com/docs/guides/completion/prompt-design>
.IP \[bu] 2
<https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md>
.PP
See detailed info on settings for each endpoint at:
.IP \[bu] 2
<https://platform.openai.com/docs/>
.SS ESCAPING NEW LINES AND TABS
Input sequences \[lq]\f[I]\[rs]n\f[R]\[rq] and
\[lq]\f[I]\[rs]t\f[R]\[rq] are only treated specially in restart, start
and stop sequences!
.SS CUSTOM / AWESOME PROMPTS
When the argument to \f[CR]option -S\f[R] starts with a full stop, such
as \[lq]\f[CR]-S\f[R] \f[CR].\f[R]\f[I]my_prompt\f[R]\[rq], load, search
for, or create \f[I]my_prompt\f[R] prompt file.
If two full stops are prepended to the prompt name, load it silently.
If a comma is used instead, such as \[lq]\f[CR]-S\f[R]
\f[CR],\f[R]\f[I]my_prompt\f[R]\[rq], edit the prompt file, and then
load it.
.PP
When the argument to \f[CR]option -S\f[R] starts with a backslash or a
percent sign, such as \[lq]\f[CR]-S\f[R]
\f[CR]/\f[R]\f[I]linux_terminal\f[R]\[rq], search for an
\f[I]awesome-chatgpt-prompt(-zh)\f[R] (by Fatih KA and PlexPt).
Set \[lq]\f[CR]//\f[R]\[rq] or \[lq]\f[CR]%%\f[R]\[rq] to refresh local
cache.
Use with \f[I]davinci\f[R] and \f[I]gpt-3.5+\f[R] models.
.PP
These options also set corresponding history files automatically.
.PP
Please note and make sure to backup your important custom prompts!
They are located at \[lq]\f[CR]\[ti]/.cache/chatgptsh/\f[R]\[rq] with
the extension \[lq]\f[I].pr\f[R]\[rq].
.SS IMAGES / DALL-E
.SS 1. Image Generations
An image can be created given a text prompt.
A text PROMPT of the desired image(s) is required.
The maximum length is 1000 characters.
.SS 2. Image Variations
Variations of a given \f[I]IMAGE\f[R] can be generated.
The \f[I]IMAGE\f[R] to use as the basis for the variations must be a
valid PNG file, less than 4MB and square.
.SS 3. Image Edits
To edit an \f[I]IMAGE\f[R], a \f[I]MASK\f[R] file may be optionally
provided.
If \f[I]MASK\f[R] is not provided, \f[I]IMAGE\f[R] must have
transparency, which will be used as the mask.
A text prompt is required.
.SS 3.1 ImageMagick
If \f[B]ImageMagick\f[R] is available, input \f[I]IMAGE\f[R] and
\f[I]MASK\f[R] will be checked and processed to fit dimensions and other
requirements.
.SS 3.2 Transparent Colour and Fuzz
A transparent colour must be set with
\[lq]\f[CR]-\[at]\f[R][\f[I]COLOUR\f[R]]\[rq] to create the mask.
Defaults=\f[I]black\f[R].
.PP
By defaults, the \f[I]COLOUR\f[R] must be exact.
Use the \f[CR]fuzz option\f[R] to match colours that are close to the
target colour.
This can be set with \[lq]\f[CR]-\[at]\f[R][\f[I]VALUE%\f[R]]\[rq] as a
percentage of the maximum possible intensity, for example
\[lq]\f[CR]-\[at]\f[R]\f[I]10%black\f[R]\[rq].
.PP
See also:
.IP \[bu] 2
<https://imagemagick.org/script/color.php>
.IP \[bu] 2
<https://imagemagick.org/script/command-line-options.php#fuzz>
.SS 3.3 Mask File / Alpha Channel
An alpha channel is generated with \f[B]ImageMagick\f[R] from any image
with the set transparent colour (defaults to \f[I]black\f[R]).
In this way, it is easy to make a mask with any black and white image as
a template.
.SS 3.4 In-Paint and Out-Paint
In-painting is achieved setting an image with a MASK and a prompt.
.PP
Out-painting can also be achieved manually with the aid of this script.
Paint a portion of the outer area of an image with \f[I]alpha\f[R], or a
defined \f[I]transparent\f[R] \f[I]colour\f[R] which will be used as the
mask, and set the same \f[I]colour\f[R] in the script with
\f[CR]-\[at]\f[R].
Choose the best result amongst many results to continue the out-painting
process step-wise.
.SS AUDIO / WHISPER
.SS 1. Transcriptions
Transcribes audio file or voice record into the set language.
Set a \f[I]two-letter\f[R] \f[I]ISO-639-1\f[R] language code
(\f[I]en\f[R], \f[I]es\f[R], \f[I]ja\f[R], or \f[I]zh\f[R]) as the
positional argument following the input audio file.
A prompt may also be set as last positional parameter to help guide the
model.
This prompt should match the audio language.
.PP
If the last positional argument is \[lq].\[rq] or \[lq]last\[rq]
exactly, it will resubmit the last recorded audio input file from cache.
.PP
Note that if the audio language is different from the set language code,
output will be on the language code (translation).
.SS 2. Translations
Translates audio into \f[B]English\f[R].
An optional text to guide the model\[cq]s style or continue a previous
audio segment is optional as last positional argument.
This prompt should be in English.
.PP
Setting \f[B]temperature\f[R] has an effect, the higher the more random.
.SS ENVIRONMENT
\f[B]BLOCK_USR\f[R]
.TP
\f[B]BLOCK_USR_TTS\f[R]
Extra options for the request JSON block
(e.g.\ \[lq]\f[I]\[dq]seed\[dq]: 33, \[dq]dimensions\[dq]:
1024\f[R]\[rq]).
.TP
\f[B]CACHEDIR\f[R]
Script cache directory base.
.TP
\f[B]CHATGPTRC\f[R]
Path to the user \f[I]configuration file\f[R].
.RS
.PP
Defaults=\[dq]\f[I]\[ti]/.chatgpt.conf\f[R]\[dq]
.RE
.TP
\f[B]FILECHAT\f[R]
Path to a history / session TSV file (script-formatted).
.TP
\f[B]INSTRUCTION\f[R]
Initial initial instruction, or system message.
.TP
\f[B]INSTRUCTION_CHAT\f[R]
Initial initial instruction, or system message for chat mode.
.PP
\f[B]MOD_CHAT\f[R]
.PP
\f[B]MOD_IMAGE\f[R]
.PP
\f[B]MOD_AUDIO\f[R]
.PP
\f[B]MOD_SPEECH\f[R]
.PP
\f[B]MOD_LOCALAI\f[R]
.PP
\f[B]MOD_OLLAMA\f[R]
.PP
\f[B]MOD_MISTRAL\f[R]
.TP
\f[B]MOD_GOOGLE\f[R]
Set defaults model for each endpoint / integration.
.TP
\f[B]OLLAMA_API_HOST\f[R]
Ollama host URL (used with \f[CR]option -O\f[R]).
.PP
\f[B]OPENAI_API_HOST\f[R]
.TP
\f[B]OPENAI_API_HOST_FIXED\f[R]
Custom host URL.
The \f[I]STATIC\f[R] parameter disables endpoint auto selection.
.PP
\f[B]OPENAI_KEY\f[R]
.PP
\f[B]OPENAI_API_KEY\f[R]
.PP
\f[B]GOOGLE_API_KEY\f[R]
.TP
\f[B]MISTRAL_API_KEY\f[R]
OpenAI, GoogleAI, and MistralAI API keys.
.TP
\f[B]OUTDIR\f[R]
Output directory for received image and audio.
.PP
\f[B]RESTART\f[R]
.TP
\f[B]START\f[R]
Restart and start sequences.
May be set to \f[I]null\f[R].
.RS
.PP
Restart=\[lq]\f[I]\[rs]nQ:\ \f[R]\[rq]
Start=\[dq]\f[I]\[rs]nA:\f[R]\[dq] (chat mode)
.RE
.PP
\f[B]VISUAL\f[R]
.TP
\f[B]EDITOR\f[R]
Text editor for external prompt editing.
.RS
.PP
Defaults=\[dq]\f[I]vim\f[R]\[dq]
.RE
.TP
\f[B]CLIP_CMD\f[R]
Clipboard set command, e.g.\ \[lq]\f[I]xsel\f[R] \f[I]-b\f[R]\[rq],
\[lq]\f[I]pbcopy\f[R]\[rq].
.TP
\f[B]PLAY_CMD\f[R]
Audio player command, e.g.\ \[lq]\f[I]mpv \[en]no-video
\[en]vo=null\f[R]\[rq].
.TP
\f[B]REC_CMD\f[R]
Audio recorder command, e.g.\ \[lq]\f[I]sox -d\f[R]\[rq].
.SS COLOUR THEMES
The colour scheme may be customised.
A few themes are available in the template configuration file.
.PP
A small colour library is available for the user conf file to
personalise the theme colours.
.PP
The colour palette is composed of \f[I]$Red\f[R], \f[I]$Green\f[R],
\f[I]$Yellow\f[R], \f[I]$Blue\f[R], \f[I]$Purple\f[R], \f[I]$Cyan\f[R],
\f[I]$White\f[R], \f[I]$Inv\f[R] (invert), and \f[I]$Nc\f[R] (reset)
variables.
.PP
Bold variations are defined as \f[I]$BRed\f[R], \f[I]$BGreen\f[R], etc,
and background colours can be set with \f[I]$On_Yellow\f[R],
\f[I]$On_Blue\f[R], etc.
.PP
Alternatively, raw escaped color sequences, such as
\f[I]\[rs]e[0;35m\f[R], and \f[I]\[rs]e[1;36m\f[R] may be set.
.PP
Theme colours are named variables from \f[CR]Colour1\f[R] to about
\f[CR]Colour11\f[R], and may be set with colour-named variables or raw
escape sequences (these must not change cursor position).
.SS REQUIRED PACKAGES
.IP \[bu] 2
\f[CR]Bash\f[R]
.IP \[bu] 2
\f[CR]cURL\f[R], and \f[CR]JQ\f[R]
.SS OPTIONAL PACKAGES
Optional packages for specific features.
.IP \[bu] 2
\f[CR]Base64\f[R] - Image endpoint, vision models
.IP \[bu] 2
\f[CR]ImageMagick\f[R]/\f[CR]fbida\f[R] - Image edits and variations
.IP \[bu] 2
\f[CR]Python\f[R] - Modules tiktoken, markdown, bs4
.IP \[bu] 2
\f[CR]mpv\f[R]/\f[CR]SoX\f[R]/\f[CR]Vlc\f[R]/\f[CR]FFmpeg\f[R]/\f[CR]afplay\f[R]
- Play TTS output
.IP \[bu] 2
\f[CR]SoX\f[R]/\f[CR]Arecord\f[R]/\f[CR]FFmpeg\f[R] - Record input
(Whisper)
.IP \[bu] 2
\f[CR]xdg-open\f[R]/\f[CR]open\f[R]/\f[CR]xsel\f[R]/\f[CR]xclip\f[R]/\f[CR]pbcopy\f[R]
- Open images, set clipboard
.IP \[bu] 2
\f[CR]W3M\f[R]/\f[CR]Lynx\f[R]/\f[CR]ELinks\f[R]/\f[CR]Links\f[R] - Dump
URL text
.IP \[bu] 2
\f[CR]bat\f[R]/\f[CR]Pygmentize\f[R]/\f[CR]Glow\f[R]/\f[CR]mdcat\f[R]/\f[CR]mdless\f[R]
- Markdown support
.IP \[bu] 2
\f[CR]termux-api\f[R]/\f[CR]play-audio\f[R]/\f[CR]termux-microphone-record\f[R]/\f[CR]termux-clipboard-set\f[R]
- Termux system
.IP \[bu] 2
\f[CR]poppler\f[R]/\f[CR]gs\f[R]/\f[CR]abiword\f[R]/\f[CR]ebook-convert\f[R]
- Dump PDF as text
.IP \[bu] 2
\f[CR]dialog\f[R]/\f[CR]kdialog\f[R]/\f[CR]zenity\f[R]/\f[CR]osascript\f[R]/\f[CR]termux-dialog\f[R]
- File picker
.SS BUGS
Bash \[lq]read command\[rq] may not correctly display input buffers
larger than the TTY screen size during editing.
However, input buffers remain unaffected.
Use the text editor interface for big prompt editing.
.PP
File paths containing spaces may not work correctly with some script
features.
.PP
Bash truncates input on \[lq]\[rs]000\[rq] (null).
.PP
Garbage in, garbage out.
An idiot savant.
.SS LIMITS
The script objective is to implement most features of OpenAI API version
1 but not all endpoints, or options will be covered.
.SS OPTIONS
.SS Model Settings
.TP
\f[B]-\[at]\f[R], \f[B]--alpha\f[R] [[\f[I]VAL%\f[R]]\f[I]COLOUR\f[R]]
Set transparent colour of image mask.
Def=\f[I]black\f[R].
.RS
.PP
Fuzz intensity can be set with [\f[I]VAL%\f[R]].
Def=\f[I]0%\f[R].
.RE
.TP
\f[B]-Nill\f[R]
Unset model max response (chat cmpls only).
.PP
\f[B]-NUM\f[R]
.TP
\f[B]-M\f[R], \f[B]--max\f[R] [\f[I]NUM\f[R][\f[I]-NUM\f[R]]]
Set maximum number of \f[I]response tokens\f[R].
Def=\f[I]1024\f[R].
.RS
.PP
A second number in the argument sets model capacity.
.RE
.TP
\f[B]-N\f[R], \f[B]--modmax\f[R] [\f[I]NUM\f[R]]
Set \f[I]model capacity\f[R] tokens.
Def=\f[I]auto\f[R], Fallback=\f[I]4000\f[R].
.TP
\f[B]-a\f[R], \f[B]--presence-penalty\f[R] [\f[I]VAL\f[R]]
Set presence penalty (cmpls/chat, -2.0 - 2.0).
.TP
\f[B]-A\f[R], \f[B]--frequency-penalty\f[R] [\f[I]VAL\f[R]]
Set frequency penalty (cmpls/chat, -2.0 - 2.0).
.TP
\f[B]-b\f[R], \f[B]--best-of\f[R] [\f[I]NUM\f[R]]
Set best of, must be greater than \f[CR]option -n\f[R] (cmpls).
Def=\f[I]1\f[R].
.TP
\f[B]-B\f[R], \f[B]--logprobs\f[R] [\f[I]NUM\f[R]]
Request log probabilities, also see -Z (cmpls, 0 - 5),
.TP
\f[B]-K\f[R], \f[B]\[en]top-k\f[R] [\f[I]NUM\f[R]]
Set Top_k value (local-ai, ollama, google).
.TP
\f[B]--keep-alive\f[R], \f[B]--ka\f[R]=[\f[I]NUM\f[R]]
Set how long the model will stay loaded into memory (ollama).
.TP
\f[B]-m\f[R], \f[B]--model\f[R] [\f[I]MODEL\f[R]]
Set language \f[I]MODEL\f[R] name.
Def=\f[I]gpt-3.5-turbo-instruct\f[R], \f[I]gpt-4o\f[R].
.RS
.PP
Set \f[I]MODEL\f[R] name as \[lq]\f[I].\f[R]\[rq] to pick from the list.
.RE
.TP
\f[B]--multimodal\f[R]
Set model as multimodal.
.TP
\f[B]-n\f[R], \f[B]--results\f[R] [\f[I]NUM\f[R]]
Set number of results.
Def=\f[I]1\f[R].
.TP
\f[B]-p\f[R], \f[B]--top-p\f[R] [\f[I]VAL\f[R]]
Set Top_p value, nucleus sampling (cmpls/chat, 0.0 - 1.0).
.TP
\f[B]-r\f[R], \f[B]--restart\f[R] [\f[I]SEQ\f[R]]
Set restart sequence string (cmpls).
.TP
\f[B]-R\f[R], \f[B]--start\f[R] [\f[I]SEQ\f[R]]
Set start sequence string (cmpls).
.TP
\f[B]-s\f[R], \f[B]--stop\f[R] [\f[I]SEQ\f[R]]
Set stop sequences, up to 4.
Def=\[dq]\f[I]<|endoftext|>\f[R]\[dq].
.TP
\f[B]-S\f[R], \f[B]--instruction\f[R] [\f[I]INSTRUCTION\f[R]|\f[I]FILE\f[R]]
Set an instruction text prompt.
It may be a text file.
.TP
\f[B]-t\f[R], \f[B]--temperature\f[R] [\f[I]VAL\f[R]]
Set temperature value (cmpls/chat/whisper), (0.0 - 2.0, whisper 0.0 -
1.0).
Def=\f[I]0\f[R].
.SS Script Modes
.TP
\f[B]-c\f[R], \f[B]--chat\f[R]
Chat mode in text completions, session break.
.TP
\f[B]-cc\f[R]
Chat mode in chat completions, session break.
.TP
\f[B]-C\f[R], \f[B]--continue\f[R], \f[B]--resume\f[R]
Continue from (resume) last session (cmpls/chat).
.TP
\f[B]-d\f[R], \f[B]--text\f[R]
Start new multi-turn session in plain text completions.
.TP
\f[B]-e\f[R], \f[B]--edit\f[R]
Edit first input from stdin, or file read (cmpls/chat).
.TP
\f[B]-E\f[R], \f[B]-EE\f[R], \f[B]--exit\f[R]
Exit on first run (even with options -cc).
.TP
\f[B]-g\f[R], \f[B]--stream\f[R] (\f[I]defaults\f[R])
Set response streaming.
.TP
\f[B]-G\f[R], \f[B]--no-stream\f[R]
Unset response streaming.
.TP
\f[B]-i\f[R], \f[B]--image\f[R] [\f[I]PROMPT\f[R]]
Generate images given a prompt.
Set \f[I]option -v\f[R] to not open response.
.TP
\f[B]-i\f[R] [\f[I]PNG\f[R]]
Create variations of a given image.
.TP
\f[B]-i\f[R] [\f[I]PNG\f[R]] [\f[I]MASK\f[R]] [\f[I]PROMPT\f[R]]
Edit image with mask and prompt (required).
.TP
\f[B]-qq\f[R], \f[B]--insert\f[R] 
Insert text rather than completing only.
May be set twice for multi-turn.
.RS
.PP
Use \[lq]\f[I][insert]\f[R]\[rq] to indicate where the language model
should insert text (\f[CR]gpt-3.5-turbo-instruct+\f[R]).
.RE
.PP
\f[B]-S\f[R] \f[B].\f[R][\f[I]PROMPT_NAME\f[R]],
\f[B]-..\f[R][\f[I]PROMPT_NAME\f[R]]
.TP
\f[B]-S\f[R] \f[B],\f[R][\f[I]PROMPT_NAME\f[R]], \f[B]-,\f[R][\f[I]PROMPT_NAME\f[R]]
Load, search for, or create custom prompt.
.RS
.PP
Set \f[CR].\f[R][\f[I]PROMPT\f[R]] to single-shot edit prompt.
.PP
Set \f[CR]..\f[R][\f[I]PROMPT\f[R]] to silently load prompt.
.PP
Set \f[CR],\f[R][\f[I]PROMPT\f[R]] to edit a prompt file.
.PP
Set \f[CR].\f[R]\f[I]?\f[R], or \f[CR].\f[R]\f[I]list\f[R] to list
prompt template files.
.RE
.PP
\f[B]-S\f[R] \f[B]/\f[R][\f[I]AWESOME_PROMPT_NAME\f[R]]
.TP
\f[B]-S\f[R] \f[B]%\f[R][\f[I]AWESOME_PROMPT_NAME_ZH\f[R]]
Set or search for an \f[I]awesome-chatgpt-prompt(-zh)\f[R].
\f[I]Davinci\f[R] and \f[I]gpt3.5+\f[R] models.
.RS
.PP
Set \f[B]//\f[R] or \f[B]%%\f[R] instead to refresh cache.
.RE
.PP
\f[B]-T\f[R], \f[B]--tiktoken\f[R]
.PP
\f[B]-TT\f[R]
.TP
\f[B]-TTT\f[R]
Count input tokens with python Tiktoken (ignores special tokens).
.RS
.PP
Set twice to print tokens, thrice to available encodings.
.PP
Set the model or encoding with \f[CR]option -m\f[R].
.PP
It heeds \f[CR]options -ccm\f[R].
.RE
.TP
\f[B]-w\f[R], \f[B]--transcribe\f[R] [\f[I]AUD\f[R]] [\f[I]LANG\f[R]] [\f[I]PROMPT\f[R]]
Transcribe audio file into text.
LANG is optional.
A prompt that matches the audio language is optional.
Audio will be transcribed or translated to the target LANG.
.RS
.PP
Set twice to get phrase-level timestamps.
.RE
.TP
\f[B]-W\f[R], \f[B]--translate\f[R] [\f[I]AUD\f[R]] [\f[I]PROMPT-EN\f[R]]
Translate audio file into English text.
.RS
.PP
Set twice to get phrase-level timestamps.
.RE
.SS Script Settings
.TP
\f[B]--api-key\f[R] [\f[I]KEY\f[R]]
Set OpenAI API key.
.TP
\f[B]-f\f[R], \f[B]--no-conf\f[R]
Ignore user configuration file.
.TP
\f[B]-F\f[R]
Edit configuration file with text editor, if it exists.
.RS
.PP
$CHATGPTRC=\[dq]\f[I]\[ti]/.chatgpt.conf\f[R]\[dq].
.RE
.TP
\f[B]-FF\f[R]
Dump template configuration file to stdout.
.TP
\f[B]--fold\f[R] (\f[I]defaults\f[R]), \f[B]--no-fold\f[R]
Set or unset response folding (wrap at white spaces).
.TP
\f[B]--google\f[R]
Set Google Gemini integration (cmpls/chat).
.TP
\f[B]-h\f[R], \f[B]--help\f[R]
Print the help page.
.TP
\f[B]-H\f[R], \f[B]--hist\f[R] [\f[CR]/\f[R]\f[I]HIST_FILE\f[R]]
Edit history file with text editor or pipe to stdout.
.RS
.PP
A history file name can be optionally set as argument.
.RE
.TP
\f[B]-P\f[R], \f[B]-PP\f[R], \f[B]--print\f[R] [\f[CR]/\f[R]\f[I]HIST_FILE\f[R]]
Print out last history session.
.RS
.PP
Set twice to print commented out history entries, inclusive.
Heeds \f[CR]options -ccdrR\f[R].
.PP
These are aliases to \f[B]-HH\f[R] and \f[B]-HHH\f[R], respectively.
.RE
.TP
\f[B]-k\f[R], \f[B]--no-colour\f[R]
Disable colour output.
Def=\f[I]auto\f[R].
.TP
\f[B]-l\f[R], \f[B]--list-models\f[R] [\f[I]MODEL\f[R]]
List models or print details of \f[I]MODEL\f[R].
.TP
\f[B]-L\f[R], \f[B]--log\f[R] [\f[I]FILEPATH\f[R]]
Set log file.
\f[I]FILEPATH\f[R] is required.
.TP
\f[B]--localai\f[R]
Set LocalAI integration (cmpls/chat).
.TP
\f[B]--mistral\f[R]
Set Mistral AI integration (chat).
.TP
\f[B]--md\f[R], \f[B]--markdown\f[R], \f[B]--markdown\f[R]=[\f[I]SOFTWARE\f[R]]
Enable markdown rendering in response.
Software is optional: \f[I]bat\f[R], \f[I]pygmentize\f[R],
\f[I]glow\f[R], \f[I]mdcat\f[R], or \f[I]mdless\f[R].
.TP
\f[B]--no-md\f[R], \f[B]--no-markdown\f[R]
Disable markdown rendering.
.TP
\f[B]-o\f[R], \f[B]--clipboard\f[R]
Copy response to clipboard.
.TP
\f[B]-O\f[R], \f[B]--ollama\f[R]
Set and make requests to Ollama server (cmpls/chat).
.TP
\f[B]-u\f[R], \f[B]--multiline\f[R]
Toggle multiline prompter, <\f[I]CTRL-D\f[R]> flush.
.TP
\f[B]-U\f[R], \f[B]--cat\f[R]
Set cat prompter, <\f[I]CTRL-D\f[R]> flush.
.TP
\f[B]-v\f[R], \f[B]--verbose\f[R]
Less verbose.
Sleep after response in voice chat (\f[CR]-vvccw\f[R]).
May be set multiple times.
.TP
\f[B]-V\f[R], \f[B]-VV\f[R]
Pretty-print all context before request.
.RS
.PP
Set twice to dump raw request block (debug).
.RE
.TP
\f[B]-x\f[R], \f[B]--editor\f[R]
Edit prompt in text editor.
.TP
\f[B]-y\f[R], \f[B]--tik\f[R]
Set tiktoken for token count (cmpls/chat, python).
.TP
\f[B]-Y\f[R], \f[B]--no-tik\f[R] (\f[I]defaults\f[R])
Unset tiktoken use (cmpls/chat, python).
.TP
\f[B]-z\f[R], \f[B]--tts\f[R] [\f[I]OUTFILE\f[R]|\f[I]FORMAT\f[R]|\f[I]-\f[R]] [\f[I]VOICE\f[R]] [\f[I]SPEED\f[R]] [\f[I]PROMPT\f[R]]
Synthesise speech from text prompt.
Takes a voice name, speed and text prompt.
Set \f[I]option -v\f[R] to not play response.
.TP
\f[B]-Z\f[R], \f[B]--last\f[R]
Print last response JSON data.
.SH AUTHORS
mountaineerbr.
